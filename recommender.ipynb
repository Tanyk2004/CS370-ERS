{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BkUgvTwc7l7O"
      },
      "source": [
        "## Educational Recommender System\n",
        "---\n",
        "\n",
        "Dataset: [Open University Learning Analytics dataset](https://analyse.kmi.open.ac.uk/open_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "zDCht_za7l7R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.metrics import accuracy_score, mean_squared_error\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "Q_58YldS7l7T"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cbgvv0NF8Zdu",
        "outputId": "6cd29a7a-cdd8-4b00-8119-cfcc21b4c319"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDYW899D8bdz",
        "outputId": "ad64ff10-dc53-4cc4-d1b1-68af480205e8"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# \n",
        "# path = os.path.join(\"drive\", \"MyDrive\")\n",
        "# path += \"/\"\n",
        "# print(path)\n",
        "path = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "BNLmHjkO7l7T",
        "outputId": "e3193de8-b047-4111-e56d-6e7174f109a5"
      },
      "outputs": [],
      "source": [
        "# load csv files into dataframes\n",
        "assessments_df = pd.read_csv(path + 'data/assessments.csv')\n",
        "courses_df = pd.read_csv(path + 'data/courses.csv')\n",
        "student_assessment_df = pd.read_csv(path + 'data/studentAssessment.csv')\n",
        "student_info_df = pd.read_csv(path + 'data/studentInfo.csv')\n",
        "student_reg_df = pd.read_csv(path + 'data/studentRegistration.csv')\n",
        "student_vle_df = pd.read_csv(path + 'data/studentVle.csv')\n",
        "vle_df = pd.read_csv(path + 'data/vle.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6364,)"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "unique_vle_resources = vle_df['id_site'].unique()\n",
        "unique_vle_resources.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "NNb3K6ea7l7U"
      },
      "outputs": [],
      "source": [
        "# assessments_df, test_assessments_df = train_test_split(assessments_df, test_size=0.2, random_state=42)\n",
        "# courses_df, test_courses_df = train_test_split(courses_df, test_size=0.2, random_state=42)\n",
        "# student_assessment_df, test_student_assessment_df = train_test_split(student_assessment_df, test_size=0.2, random_state=42)\n",
        "# student_info_df, test_student_info_df = train_test_split(student_info_df, test_size=0.2, random_state=42)\n",
        "# student_reg_df, test_student_reg_df = train_test_split(student_reg_df, test_size=0.2, random_state=42)\n",
        "# student_vle_df, test_student_vle_df = train_test_split(student_vle_df, test_size=0.2, random_state=42)\n",
        "# vle_df, test_vle_df = train_test_split(vle_df, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(6268,)"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "student_vle_df['id_site'].unique().shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "M3GqSvsz7l7V",
        "outputId": "aec4e0b9-95b0-4752-f193-20c6d28ccd78"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code_module</th>\n",
              "      <th>code_presentation</th>\n",
              "      <th>id_assessment</th>\n",
              "      <th>assessment_type</th>\n",
              "      <th>date</th>\n",
              "      <th>weight</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AAA</td>\n",
              "      <td>2013J</td>\n",
              "      <td>1752</td>\n",
              "      <td>TMA</td>\n",
              "      <td>19.0</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AAA</td>\n",
              "      <td>2013J</td>\n",
              "      <td>1753</td>\n",
              "      <td>TMA</td>\n",
              "      <td>54.0</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AAA</td>\n",
              "      <td>2013J</td>\n",
              "      <td>1754</td>\n",
              "      <td>TMA</td>\n",
              "      <td>117.0</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AAA</td>\n",
              "      <td>2013J</td>\n",
              "      <td>1755</td>\n",
              "      <td>TMA</td>\n",
              "      <td>166.0</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AAA</td>\n",
              "      <td>2013J</td>\n",
              "      <td>1756</td>\n",
              "      <td>TMA</td>\n",
              "      <td>215.0</td>\n",
              "      <td>30.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  code_module code_presentation  id_assessment assessment_type   date  weight\n",
              "0         AAA             2013J           1752             TMA   19.0    10.0\n",
              "1         AAA             2013J           1753             TMA   54.0    20.0\n",
              "2         AAA             2013J           1754             TMA  117.0    20.0\n",
              "3         AAA             2013J           1755             TMA  166.0    20.0\n",
              "4         AAA             2013J           1756             TMA  215.0    30.0"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "assessments_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M18odsdn7l7W"
      },
      "source": [
        "### Removing Bias\n",
        "\n",
        "We need to remove certain columns in the student dataset so that recommendations out of our recommender system are not biased in any way.\n",
        "\n",
        "We will definitely be removing\n",
        "- Gender\n",
        "- Region\n",
        "\n",
        "We will be testing\n",
        "- Age\n",
        "- Economic Land\n",
        "- Previous Attempts\n",
        "- Disability\n",
        "- Number of Credits in the modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "NNXyhmXZ7l7W",
        "outputId": "a6946162-fd14-4e3d-a3e0-cba336142a0f"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code_module</th>\n",
              "      <th>code_presentation</th>\n",
              "      <th>id_student</th>\n",
              "      <th>highest_education</th>\n",
              "      <th>imd_band</th>\n",
              "      <th>age_band</th>\n",
              "      <th>num_of_prev_attempts</th>\n",
              "      <th>studied_credits</th>\n",
              "      <th>disability</th>\n",
              "      <th>final_result</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AAA</td>\n",
              "      <td>2013J</td>\n",
              "      <td>11391</td>\n",
              "      <td>HE Qualification</td>\n",
              "      <td>90-100%</td>\n",
              "      <td>55&lt;=</td>\n",
              "      <td>0</td>\n",
              "      <td>240</td>\n",
              "      <td>N</td>\n",
              "      <td>Pass</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AAA</td>\n",
              "      <td>2013J</td>\n",
              "      <td>28400</td>\n",
              "      <td>HE Qualification</td>\n",
              "      <td>20-30%</td>\n",
              "      <td>35-55</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "      <td>N</td>\n",
              "      <td>Pass</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AAA</td>\n",
              "      <td>2013J</td>\n",
              "      <td>30268</td>\n",
              "      <td>A Level or Equivalent</td>\n",
              "      <td>30-40%</td>\n",
              "      <td>35-55</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "      <td>Y</td>\n",
              "      <td>Withdrawn</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AAA</td>\n",
              "      <td>2013J</td>\n",
              "      <td>31604</td>\n",
              "      <td>A Level or Equivalent</td>\n",
              "      <td>50-60%</td>\n",
              "      <td>35-55</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "      <td>N</td>\n",
              "      <td>Pass</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AAA</td>\n",
              "      <td>2013J</td>\n",
              "      <td>32885</td>\n",
              "      <td>Lower Than A Level</td>\n",
              "      <td>50-60%</td>\n",
              "      <td>0-35</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "      <td>N</td>\n",
              "      <td>Pass</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  code_module code_presentation  id_student      highest_education imd_band  \\\n",
              "0         AAA             2013J       11391       HE Qualification  90-100%   \n",
              "1         AAA             2013J       28400       HE Qualification   20-30%   \n",
              "2         AAA             2013J       30268  A Level or Equivalent   30-40%   \n",
              "3         AAA             2013J       31604  A Level or Equivalent   50-60%   \n",
              "4         AAA             2013J       32885     Lower Than A Level   50-60%   \n",
              "\n",
              "  age_band  num_of_prev_attempts  studied_credits disability final_result  \n",
              "0     55<=                     0              240          N         Pass  \n",
              "1    35-55                     0               60          N         Pass  \n",
              "2    35-55                     0               60          Y    Withdrawn  \n",
              "3    35-55                     0               60          N         Pass  \n",
              "4     0-35                     0               60          N         Pass  "
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "student_info_df.drop(inplace=True, columns=['gender', 'region'])\n",
        "student_info_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "oFX3qZhO7l7W"
      },
      "outputs": [],
      "source": [
        "# divide the courses into B and J categories based on \"code_presentation\" because the structure of these\n",
        "# presentations is different\n",
        "\n",
        "# student_vle_df = pd.merge(student_vle_df, vle_df, on='id_site')\n",
        "\n",
        "# * Pre processing Assessments\n",
        "courses_B_df = courses_df[courses_df['code_presentation'].str.contains('B')]\n",
        "courses_J_df = courses_df[courses_df['code_presentation'].str.contains('J')]\n",
        "\n",
        "# * Pre processing Assessments\n",
        "student_assessment_df = pd.merge(student_assessment_df, assessments_df, on='id_assessment')\n",
        "student_assessment_df = pd.get_dummies(student_assessment_df, columns=[\"assessment_type\", \"code_presentation\"])\n",
        "assessments_df.dropna(how='any', inplace=True)\n",
        "assessments_df = pd.get_dummies(assessments_df, columns=[\"assessment_type\"])\n",
        "assessments_df[\"date\"] = [int(date / 7) for date in assessments_df[\"date\"]]\n",
        "assessments_B_df = assessments_df[assessments_df['code_module'].isin(courses_B_df['code_module'])]\n",
        "assessments_J_df = assessments_df[assessments_df['code_module'].isin(courses_J_df['code_module'])]\n",
        "\n",
        "# * Pre processing VLE Dataset\n",
        "# ! This gets rid of the records with no week_from and week_to values\n",
        "# ! However, we are not sure if this is the right way to handle this\n",
        "# * Maybe we can use the student VLE interactions to fill in the missing values\n",
        "\n",
        "vle_df.drop(columns=['week_from', 'week_to'], inplace=True)\n",
        "vle_df = pd.get_dummies(vle_df, columns=[\"activity_type\"])\n",
        "vle_B_df = vle_df[vle_df['code_module'].isin(courses_B_df['code_module'])]\n",
        "vle_J_df = vle_df[vle_df['code_module'].isin(courses_J_df['code_module'])]\n",
        "\n",
        "\n",
        "# * Pre processing Student Info Dataset\n",
        "student_info_df.dropna(how='any', inplace=True)\n",
        "student_info_df = pd.get_dummies(student_info_df, columns=[\"highest_education\", \"imd_band\", \"age_band\", \"disability\", \"final_result\"])\n",
        "student_info_B_df = student_info_df[student_info_df['code_module'].isin(courses_B_df['code_module'])]\n",
        "student_info_J_df = student_info_df[student_info_df['code_module'].isin(courses_J_df['code_module'])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "tcqeveDq7l7X"
      },
      "outputs": [],
      "source": [
        "def get_vle_interaction_by_site(id_site):\n",
        "    \"\"\"\n",
        "    Returns the student vle interaction for a given site id\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame()\n",
        "    df = student_vle_df[student_vle_df['id_site'] == id_site]\n",
        "    return df\n",
        "\n",
        "def get_vle_interaction_by_student(student_id):\n",
        "    \"\"\"\n",
        "    Returns the student vle interaction for a given student id\n",
        "    \"\"\"\n",
        "    return student_vle_df[student_vle_df['id_student'] == student_id]\n",
        "\n",
        "def get_student_assessment_scores(student_id):\n",
        "    \"\"\"\n",
        "    Returns the student assessment scores for a given student id\n",
        "    \"\"\"\n",
        "    df = pd.DataFrame()\n",
        "    df = student_assessment_df[student_assessment_df['id_student'] == student_id]\n",
        "    return df\n",
        "\n",
        "def get_unique_vle_sites():\n",
        "    return student_vle_df['id_site'].unique()\n",
        "\n",
        "def calculate_euclidean_distance_student(student_id):\n",
        "    \"\"\"\n",
        "    Returns the euclidean distance between the student and all other students\n",
        "    \"\"\"\n",
        "    student = student_info_df[student_info_df['id_student'] == student_id]\n",
        "    student = student.drop(columns=['id_student'])\n",
        "    student = student.values\n",
        "    students = student_info_df.drop(columns=['id_student', 'code_module', 'code_presentation'])\n",
        "    students = students.values\n",
        "    return np.linalg.norm(students - student, axis=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "WWJ-dl8a7l7X",
        "outputId": "4a6f0dfd-b637-4bff-e6de-c9cb14e89910"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(30757, 29)"
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "student_info_B_df.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yI6Poyx7l7X"
      },
      "source": [
        "### Data Exploration\n",
        "\n",
        "We need to find trends in students' engagement with the VLE and their test scores (indication of proficiency in a topic).It would appear that  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "fxs3KSSx7l7Y",
        "outputId": "de1f8040-c463-4abf-dc2f-6fe3f9c597f0"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
        "# find a correlation between VLE Clicks and Student assesments\n",
        "\n",
        "def get_clicks_for_id_site_by_student(id_site, student_vle_interaction):\n",
        "    \"\"\"\n",
        "    Returns the number of clicks for a given site id\n",
        "    \"\"\"\n",
        "    return student_vle_interaction[student_vle_interaction['id_site'] == id_site].sum()[\"sum_click\"]\n",
        "\n",
        "def get_weighted_average_score(student_id):\n",
        "    \"\"\"\n",
        "    Returns the weighted average score for a given student\n",
        "    \"\"\"\n",
        "    assessment_scores = get_student_assessment_scores(student_id)\n",
        "    assessment_scores = pd.merge(assessment_scores, assessments_B_df, on='id_assessment')\n",
        "    average_score = 0\n",
        "    for index, row in assessment_scores.iterrows():\n",
        "        average_score += (row[\"weight_x\"] / 100) * row[\"score\"]\n",
        "    sum_assessment_weights = assessment_scores[\"weight_x\"].sum()\n",
        "    average_score = average_score / sum_assessment_weights\n",
        "    return average_score\n",
        "\n",
        "def corr_clicks_assessments():\n",
        "    \"\"\"\n",
        "    Displays the correlation between VLE Clicks and Student assesments\n",
        "\n",
        "    ! Caution: We are only analyzing the B presentations for now\n",
        "    \"\"\"\n",
        "    clicks_assessment_scores_df = pd.DataFrame()\n",
        "\n",
        "    SAMPLE_SIZE = 1000\n",
        "\n",
        "    for index, student in tqdm(student_info_B_df.sample(n=SAMPLE_SIZE).iterrows(), total=SAMPLE_SIZE, desc=\"Processing Students\"):\n",
        "        assessment_scores = get_student_assessment_scores(student[\"id_student\"])\n",
        "        student_vle_interaction = get_vle_interaction_by_student(student[\"id_student\"])\n",
        "\n",
        "        assessment_scores = pd.merge(assessment_scores, assessments_B_df, on='id_assessment')\n",
        "\n",
        "        sum_clicks_per_student = 0\n",
        "        for id_site in student_vle_interaction[\"id_site\"].unique():\n",
        "            clicks = get_clicks_for_id_site_by_student(id_site, student_vle_interaction)\n",
        "            sum_clicks_per_student += clicks\n",
        "\n",
        "        average_score = 0\n",
        "        for index, row in assessment_scores.iterrows():\n",
        "            average_score += (row[\"weight_x\"] / 100) * row[\"score\"]\n",
        "        sum_assessment_weights = assessment_scores[\"weight_x\"].sum()\n",
        "        average_score = average_score / sum_assessment_weights\n",
        "        new_row = pd.DataFrame({\"sum_clicks\": [sum_clicks_per_student], \"average_score\": [average_score]})\n",
        "        if (sum_clicks_per_student < 1500):\n",
        "            clicks_assessment_scores_df = pd.concat([clicks_assessment_scores_df, new_row], ignore_index=True)\n",
        "\n",
        "    # fit a least squares line\n",
        "    X = clicks_assessment_scores_df[\"sum_clicks\"].values.reshape(-1, 1)\n",
        "    y = clicks_assessment_scores_df[\"average_score\"].values.reshape(-1, 1)\n",
        "\n",
        "    # drop NaN values from y and drop the corresponding x values\n",
        "\n",
        "    nan_indices = np.isnan(y)\n",
        "\n",
        "    X = X[~nan_indices]\n",
        "    y = y[~nan_indices]\n",
        "\n",
        "    X = X.reshape(-1, 1)\n",
        "    y = y.reshape(-1, 1)\n",
        "\n",
        "    model = LinearRegression().fit(X, y)\n",
        "    m = model.coef_[0][0]\n",
        "    b = model.intercept_[0]\n",
        "\n",
        "    plt.scatter(clicks_assessment_scores_df[\"sum_clicks\"], clicks_assessment_scores_df[\"average_score\"])\n",
        "    plt.plot(X, m * X + b, color='red')\n",
        "    plt.xlabel(\"Sum Clicks\")\n",
        "    plt.ylabel(\"Average Score\")\n",
        "    plt.title(\"Correlation between Sum Clicks and Average Score\")\n",
        "    plt.show()\n",
        "\n",
        "    correlation_clicks_average_score = clicks_assessment_scores_df[\"sum_clicks\"].corr(clicks_assessment_scores_df[\"average_score\"])\n",
        "    print(f\"Correlation between Sum Clicks and Average Score: {correlation_clicks_average_score}\")\n",
        "\n",
        "\n",
        "\n",
        "# corr_clicks_assessments()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFQcxO2G7l7Y"
      },
      "source": [
        "### Clicks vs Type of Content\n",
        "\n",
        "We needed to see if the sum_clicks differ drastically based on content type in the VLE. It would appear that it does. There seems to be steep difference in the sum_clicks on \"oucontent\" type vs something like \"dataplus\". OUContent appears to be content embedded within the Open University VLE itself and the abnormally large sum_clicks could be due to a tracking deficit in other activity types.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "Gs1hcSGr7l7Y",
        "outputId": "95a75ad7-4d47-40cc-b9c3-f56da171ec53"
      },
      "outputs": [],
      "source": [
        "# Get distribution of clicks by type of activity\n",
        "\n",
        "def show_clicks_by_type():\n",
        "    \"\"\"\n",
        "    Displays the distribution of clicks by type of activity\n",
        "    \"\"\"\n",
        "    global student_vle_df\n",
        "    merged_vle = pd.merge(student_vle_df, vle_df, on='id_site')\n",
        "    # print(merged_vle.head())\n",
        "    activity_names = merged_vle.columns[10:]\n",
        "    activity_names = pd.Series(activity_names)\n",
        "    for i in range(len(activity_names)):\n",
        "        activity_names[i] = activity_names[i].replace(\"activity_type_\", \"\")\n",
        "    clicks_by_type = np.zeros(shape=(len(activity_names), 1))\n",
        "\n",
        "    # Add clicks to clicks by type based on which activity column has True\n",
        "    activity_columns = merged_vle.columns[10:]\n",
        "    clicks_by_type = merged_vle[activity_columns].multiply(merged_vle[\"sum_click\"], axis=\"index\").sum(axis=0).values\n",
        "    clicks_by_type = pd.DataFrame(clicks_by_type, index=activity_names, columns=[\"sum_click\"])\n",
        "\n",
        "    clicks_by_type.plot(kind='bar')\n",
        "    plt.xlabel(\"Activity Type\")\n",
        "    plt.ylabel(\"Sum Clicks\")\n",
        "    plt.title(\"Distribution of Clicks by Type of Activity\")\n",
        "    plt.show()\n",
        "\n",
        "# show_clicks_by_type()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "cQF0WQJp7l7Y",
        "outputId": "7d644767-1ddf-4b40-d019-419ef0ea1148"
      },
      "outputs": [],
      "source": [
        "# Get how many of each resource is there\n",
        "def get_resource_distribution():\n",
        "    \"\"\"\n",
        "    Returns the distribution of resources\n",
        "    \"\"\"\n",
        "\n",
        "    global student_vle_df\n",
        "    # student_vle_df = pd.merge(student_vle_df, vle_df, on='id_site')\n",
        "    merged_vle = pd.merge(student_vle_df, vle_df, on='id_site')\n",
        "    activity_names = merged_vle.columns[10:]\n",
        "    activity_names = pd.Series(activity_names)\n",
        "    for i in range(len(activity_names)):\n",
        "        activity_names[i] = activity_names[i].replace(\"activity_type_\", \"\")\n",
        "\n",
        "    activity_vectors = np.array(merged_vle.iloc[:, 10:], dtype=np.float64)\n",
        "    clicks_by_type = activity_vectors.sum(axis=0)\n",
        "    clicks_by_type = pd.DataFrame(clicks_by_type, index=activity_names, columns=[\"sum_click\"])\n",
        "    clicks_by_type.plot(kind='bar')\n",
        "    plt.xlabel(\"Activity Type\")\n",
        "    plt.ylabel(\"Sum Clicks\")\n",
        "    plt.title(\"Distribution of Clicks by Type of Activity\")\n",
        "    plt.show()\n",
        "\n",
        "# get_resource_distribution()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "1nDNYrjv7l7Z",
        "outputId": "fc0081cf-56b9-44b9-e722-29019d307ef3"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "import datetime\n",
        "import numpy as np\n",
        "\n",
        "# Get sucessful student ids\n",
        "def get_student_assessment_trends_slope(student_info_dataframe):\n",
        "    \"\"\"\n",
        "    Returns the ids of successful students\n",
        "    Currently the heuristic is to find students with a positive slope in their assessment scores\n",
        "    The slope threshold currently is 0.05 because that is the 75th percentile of the slope distribution\n",
        "    \"\"\"\n",
        "    # plot student assessment trends\n",
        "    student_assessment_df_merged = pd.merge(student_assessment_df, assessments_df, on='id_assessment')\n",
        "    student_trend_df = pd.DataFrame()\n",
        "\n",
        "    for index, student in tqdm(student_info_dataframe.iterrows(), total=student_info_dataframe.shape[0], desc=\"Processing Students\"):\n",
        "        student_scores = get_student_assessment_scores(student[\"id_student\"])\n",
        "\n",
        "        if (student_scores.empty):\n",
        "            continue\n",
        "\n",
        "        student_scores = student_scores.dropna(how='any', inplace=False)\n",
        "\n",
        "        # calculate a linear regression line for the student scores\n",
        "        x = student_scores[\"date\"].values.reshape(-1, 1)\n",
        "        y = student_scores[\"score\"].values.reshape(-1, 1)\n",
        "        if x.size == 0 or y.size == 0:\n",
        "           continue\n",
        "        model = LinearRegression()\n",
        "        model.fit(x, y)\n",
        "        m = model.coef_[0][0]\n",
        "        b = model.intercept_[0]\n",
        "        if x.size > 4:\n",
        "            new_row = pd.DataFrame([{\"slope\": m, \"intercept\": b, \"id_student\": student[\"id_student\"]}])\n",
        "            student_trend_df = pd.concat([student_trend_df, new_row], ignore_index=True)\n",
        "\n",
        "    student_trend_df = pd.merge(student_trend_df, student_info_dataframe, on='id_student')\n",
        "    return student_trend_df\n",
        "\n",
        "def get_successful_student_trends(student_info_dataframe):\n",
        "    df = get_student_assessment_trends_slope(student_info_dataframe)\n",
        "    return df[df[\"slope\"] > df[\"slope\"].describe()[\"75%\"]]\n",
        "\n",
        "# get_successful_student_trends(student_info_B_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "oAqwPhay7l7Z",
        "outputId": "134699ae-aaf9-4a63-f247-54dc951dc896"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Students: 100%|██████████| 50/50 [00:00<00:00, 682.69it/s]\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id_student</th>\n",
              "      <th>code_module</th>\n",
              "      <th>code_presentation</th>\n",
              "      <th>num_of_prev_attempts</th>\n",
              "      <th>studied_credits</th>\n",
              "      <th>highest_education_A Level or Equivalent</th>\n",
              "      <th>highest_education_HE Qualification</th>\n",
              "      <th>highest_education_Lower Than A Level</th>\n",
              "      <th>highest_education_No Formal quals</th>\n",
              "      <th>highest_education_Post Graduate Qualification</th>\n",
              "      <th>...</th>\n",
              "      <th>imd_band_90-100%</th>\n",
              "      <th>age_band_0-35</th>\n",
              "      <th>age_band_35-55</th>\n",
              "      <th>age_band_55&lt;=</th>\n",
              "      <th>disability_N</th>\n",
              "      <th>disability_Y</th>\n",
              "      <th>final_result_Distinction</th>\n",
              "      <th>final_result_Fail</th>\n",
              "      <th>final_result_Pass</th>\n",
              "      <th>final_result_Withdrawn</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>651627</td>\n",
              "      <td>DDD</td>\n",
              "      <td>2014J</td>\n",
              "      <td>0</td>\n",
              "      <td>90</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>684971</td>\n",
              "      <td>BBB</td>\n",
              "      <td>2014J</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>627027</td>\n",
              "      <td>BBB</td>\n",
              "      <td>2014B</td>\n",
              "      <td>0</td>\n",
              "      <td>60</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2252963</td>\n",
              "      <td>GGG</td>\n",
              "      <td>2014B</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>613646</td>\n",
              "      <td>GGG</td>\n",
              "      <td>2014B</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 29 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    id_student code_module code_presentation  num_of_prev_attempts  \\\n",
              "17      651627         DDD             2014J                     0   \n",
              "16      684971         BBB             2014J                     0   \n",
              "23      627027         BBB             2014B                     0   \n",
              "3      2252963         GGG             2014B                     0   \n",
              "22      613646         GGG             2014B                     0   \n",
              "\n",
              "    studied_credits  highest_education_A Level or Equivalent  \\\n",
              "17               90                                     True   \n",
              "16               60                                    False   \n",
              "23               60                                    False   \n",
              "3                30                                    False   \n",
              "22               30                                    False   \n",
              "\n",
              "    highest_education_HE Qualification  highest_education_Lower Than A Level  \\\n",
              "17                               False                                 False   \n",
              "16                               False                                  True   \n",
              "23                               False                                  True   \n",
              "3                                False                                  True   \n",
              "22                               False                                  True   \n",
              "\n",
              "    highest_education_No Formal quals  \\\n",
              "17                              False   \n",
              "16                              False   \n",
              "23                              False   \n",
              "3                               False   \n",
              "22                              False   \n",
              "\n",
              "    highest_education_Post Graduate Qualification  ...  imd_band_90-100%  \\\n",
              "17                                          False  ...             False   \n",
              "16                                          False  ...             False   \n",
              "23                                          False  ...             False   \n",
              "3                                           False  ...             False   \n",
              "22                                          False  ...             False   \n",
              "\n",
              "    age_band_0-35  age_band_35-55  age_band_55<=  disability_N  disability_Y  \\\n",
              "17           True           False          False          True         False   \n",
              "16          False            True          False          True         False   \n",
              "23           True           False          False          True         False   \n",
              "3           False            True          False          True         False   \n",
              "22          False            True          False          True         False   \n",
              "\n",
              "    final_result_Distinction  final_result_Fail  final_result_Pass  \\\n",
              "17                     False              False               True   \n",
              "16                     False               True              False   \n",
              "23                     False              False               True   \n",
              "3                      False              False               True   \n",
              "22                      True              False              False   \n",
              "\n",
              "    final_result_Withdrawn  \n",
              "17                   False  \n",
              "16                   False  \n",
              "23                   False  \n",
              "3                    False  \n",
              "22                   False  \n",
              "\n",
              "[5 rows x 29 columns]"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def knn_recommender(id_student, selective_student_info_dataframe, k=5):\n",
        "    \"\"\"\n",
        "    ### Parameters\n",
        "        - id_student: The student id for which we want to find similar students\n",
        "        - selective_student_info_dataframe: The dataframe containing 'successful' students\n",
        "\n",
        "    ### Returns\n",
        "        Returns the top k similar students to the given student id\n",
        "    \"\"\"\n",
        "    student = student_info_df[student_info_df['id_student'] == id_student]\n",
        "    student = student.drop(columns=['id_student', 'code_module', 'code_presentation'])\n",
        "    student = student.iloc[0]\n",
        "    students = selective_student_info_dataframe.drop(columns=['id_student', 'code_module', 'code_presentation'])\n",
        "    student = np.array(student, dtype=float)\n",
        "    students = np.array(students, dtype=float)\n",
        "    distances = np.linalg.norm(students - student, axis=1)\n",
        "    indices = np.argsort(distances)[:k]\n",
        "    return selective_student_info_dataframe.iloc[indices]\n",
        "\n",
        "successful_students = get_successful_student_trends(student_info_df.sample(n=50))\n",
        "successful_students.drop(columns=['slope', 'intercept'], inplace=True)\n",
        "knn_recommender(student_info_df.sample(n=1).iloc[0][\"id_student\"], successful_students, k=5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "0C9WsXj07l7Z",
        "outputId": "3b435eb5-a552-4273-ed63-618eba5df3b4"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>code_module_x</th>\n",
              "      <th>code_presentation_x</th>\n",
              "      <th>id_student</th>\n",
              "      <th>id_site</th>\n",
              "      <th>date</th>\n",
              "      <th>sum_click</th>\n",
              "      <th>code_module_y</th>\n",
              "      <th>code_presentation_y</th>\n",
              "      <th>activity_type_dataplus</th>\n",
              "      <th>activity_type_dualpane</th>\n",
              "      <th>...</th>\n",
              "      <th>activity_type_ouelluminate</th>\n",
              "      <th>activity_type_ouwiki</th>\n",
              "      <th>activity_type_page</th>\n",
              "      <th>activity_type_questionnaire</th>\n",
              "      <th>activity_type_quiz</th>\n",
              "      <th>activity_type_repeatactivity</th>\n",
              "      <th>activity_type_resource</th>\n",
              "      <th>activity_type_sharedsubpage</th>\n",
              "      <th>activity_type_subpage</th>\n",
              "      <th>activity_type_url</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AAA</td>\n",
              "      <td>2013J</td>\n",
              "      <td>28400</td>\n",
              "      <td>546652</td>\n",
              "      <td>-10</td>\n",
              "      <td>4</td>\n",
              "      <td>AAA</td>\n",
              "      <td>2013J</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>AAA</td>\n",
              "      <td>2013J</td>\n",
              "      <td>28400</td>\n",
              "      <td>546652</td>\n",
              "      <td>-10</td>\n",
              "      <td>1</td>\n",
              "      <td>AAA</td>\n",
              "      <td>2013J</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>AAA</td>\n",
              "      <td>2013J</td>\n",
              "      <td>28400</td>\n",
              "      <td>546652</td>\n",
              "      <td>-10</td>\n",
              "      <td>1</td>\n",
              "      <td>AAA</td>\n",
              "      <td>2013J</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>AAA</td>\n",
              "      <td>2013J</td>\n",
              "      <td>28400</td>\n",
              "      <td>546614</td>\n",
              "      <td>-10</td>\n",
              "      <td>11</td>\n",
              "      <td>AAA</td>\n",
              "      <td>2013J</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AAA</td>\n",
              "      <td>2013J</td>\n",
              "      <td>28400</td>\n",
              "      <td>546714</td>\n",
              "      <td>-10</td>\n",
              "      <td>1</td>\n",
              "      <td>AAA</td>\n",
              "      <td>2013J</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10655275</th>\n",
              "      <td>GGG</td>\n",
              "      <td>2014J</td>\n",
              "      <td>675811</td>\n",
              "      <td>896943</td>\n",
              "      <td>269</td>\n",
              "      <td>3</td>\n",
              "      <td>GGG</td>\n",
              "      <td>2014J</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10655276</th>\n",
              "      <td>GGG</td>\n",
              "      <td>2014J</td>\n",
              "      <td>675578</td>\n",
              "      <td>896943</td>\n",
              "      <td>269</td>\n",
              "      <td>1</td>\n",
              "      <td>GGG</td>\n",
              "      <td>2014J</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10655277</th>\n",
              "      <td>GGG</td>\n",
              "      <td>2014J</td>\n",
              "      <td>654064</td>\n",
              "      <td>896943</td>\n",
              "      <td>269</td>\n",
              "      <td>3</td>\n",
              "      <td>GGG</td>\n",
              "      <td>2014J</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10655278</th>\n",
              "      <td>GGG</td>\n",
              "      <td>2014J</td>\n",
              "      <td>654064</td>\n",
              "      <td>896939</td>\n",
              "      <td>269</td>\n",
              "      <td>1</td>\n",
              "      <td>GGG</td>\n",
              "      <td>2014J</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10655279</th>\n",
              "      <td>GGG</td>\n",
              "      <td>2014J</td>\n",
              "      <td>654064</td>\n",
              "      <td>896939</td>\n",
              "      <td>269</td>\n",
              "      <td>1</td>\n",
              "      <td>GGG</td>\n",
              "      <td>2014J</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>...</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10655280 rows × 28 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         code_module_x code_presentation_x  id_student  id_site  date  \\\n",
              "0                  AAA               2013J       28400   546652   -10   \n",
              "1                  AAA               2013J       28400   546652   -10   \n",
              "2                  AAA               2013J       28400   546652   -10   \n",
              "3                  AAA               2013J       28400   546614   -10   \n",
              "4                  AAA               2013J       28400   546714   -10   \n",
              "...                ...                 ...         ...      ...   ...   \n",
              "10655275           GGG               2014J      675811   896943   269   \n",
              "10655276           GGG               2014J      675578   896943   269   \n",
              "10655277           GGG               2014J      654064   896943   269   \n",
              "10655278           GGG               2014J      654064   896939   269   \n",
              "10655279           GGG               2014J      654064   896939   269   \n",
              "\n",
              "          sum_click code_module_y code_presentation_y  activity_type_dataplus  \\\n",
              "0                 4           AAA               2013J                   False   \n",
              "1                 1           AAA               2013J                   False   \n",
              "2                 1           AAA               2013J                   False   \n",
              "3                11           AAA               2013J                   False   \n",
              "4                 1           AAA               2013J                   False   \n",
              "...             ...           ...                 ...                     ...   \n",
              "10655275          3           GGG               2014J                   False   \n",
              "10655276          1           GGG               2014J                   False   \n",
              "10655277          3           GGG               2014J                   False   \n",
              "10655278          1           GGG               2014J                   False   \n",
              "10655279          1           GGG               2014J                   False   \n",
              "\n",
              "          activity_type_dualpane  ...  activity_type_ouelluminate  \\\n",
              "0                          False  ...                       False   \n",
              "1                          False  ...                       False   \n",
              "2                          False  ...                       False   \n",
              "3                          False  ...                       False   \n",
              "4                          False  ...                       False   \n",
              "...                          ...  ...                         ...   \n",
              "10655275                   False  ...                       False   \n",
              "10655276                   False  ...                       False   \n",
              "10655277                   False  ...                       False   \n",
              "10655278                   False  ...                       False   \n",
              "10655279                   False  ...                       False   \n",
              "\n",
              "          activity_type_ouwiki  activity_type_page  \\\n",
              "0                        False               False   \n",
              "1                        False               False   \n",
              "2                        False               False   \n",
              "3                        False               False   \n",
              "4                        False               False   \n",
              "...                        ...                 ...   \n",
              "10655275                 False               False   \n",
              "10655276                 False               False   \n",
              "10655277                 False               False   \n",
              "10655278                 False               False   \n",
              "10655279                 False               False   \n",
              "\n",
              "          activity_type_questionnaire  activity_type_quiz  \\\n",
              "0                               False               False   \n",
              "1                               False               False   \n",
              "2                               False               False   \n",
              "3                               False               False   \n",
              "4                               False               False   \n",
              "...                               ...                 ...   \n",
              "10655275                        False               False   \n",
              "10655276                        False               False   \n",
              "10655277                        False               False   \n",
              "10655278                        False               False   \n",
              "10655279                        False               False   \n",
              "\n",
              "          activity_type_repeatactivity  activity_type_resource  \\\n",
              "0                                False                   False   \n",
              "1                                False                   False   \n",
              "2                                False                   False   \n",
              "3                                False                   False   \n",
              "4                                False                   False   \n",
              "...                                ...                     ...   \n",
              "10655275                         False                   False   \n",
              "10655276                         False                   False   \n",
              "10655277                         False                   False   \n",
              "10655278                         False                   False   \n",
              "10655279                         False                   False   \n",
              "\n",
              "          activity_type_sharedsubpage  activity_type_subpage  \\\n",
              "0                               False                  False   \n",
              "1                               False                  False   \n",
              "2                               False                  False   \n",
              "3                               False                  False   \n",
              "4                               False                  False   \n",
              "...                               ...                    ...   \n",
              "10655275                        False                  False   \n",
              "10655276                        False                  False   \n",
              "10655277                        False                  False   \n",
              "10655278                        False                  False   \n",
              "10655279                        False                  False   \n",
              "\n",
              "          activity_type_url  \n",
              "0                     False  \n",
              "1                     False  \n",
              "2                     False  \n",
              "3                     False  \n",
              "4                     False  \n",
              "...                     ...  \n",
              "10655275              False  \n",
              "10655276              False  \n",
              "10655277              False  \n",
              "10655278              False  \n",
              "10655279              False  \n",
              "\n",
              "[10655280 rows x 28 columns]"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def prepare_DTree_data(student_features_df, resource_features_df, student_assessment_df):\n",
        "    \"\"\"\n",
        "    Prepares the data for the Decision Tree\n",
        "\n",
        "    We want to merge student features with resource features and then our target variable is the assessment score\n",
        "    \"\"\"\n",
        "\n",
        "    # merge the student features with the resource features\n",
        "    merged_df = pd.merge(student_features_df, resource_features_df, on='id_site')\n",
        "\n",
        "    return merged_df\n",
        "\n",
        "prepare_DTree_data(student_vle_df, vle_df, student_assessment_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0pD5iTMv7l7a"
      },
      "source": [
        "### Association Analysis\n",
        "\n",
        "We want to treat the resources a student consumes as items in the itemsets.\n",
        "\n",
        "TODO:\n",
        "- Create Itemsets from the resources students have consumed.\n",
        "- Append relevant information to the itemsets that indicate the impact of the itemset (average [weighted] grade or grade for the assessment that those resources were used before)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "TnoPvgkG7l7b",
        "outputId": "5565e35d-f36e-4a2d-cbdc-664424664b28"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Students: 100%|██████████| 10/10 [00:00<00:00, 76.22it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[[773425,\n",
              "  773080,\n",
              "  773459,\n",
              "  772705,\n",
              "  773452,\n",
              "  773202,\n",
              "  773122,\n",
              "  773390,\n",
              "  773141,\n",
              "  773144,\n",
              "  772735,\n",
              "  773161,\n",
              "  773182,\n",
              "  773364,\n",
              "  773340,\n",
              "  773259,\n",
              "  773034,\n",
              "  772990,\n",
              "  772987,\n",
              "  773001,\n",
              "  773234,\n",
              "  772729,\n",
              "  773455,\n",
              "  773196,\n",
              "  773172,\n",
              "  773002,\n",
              "  772988,\n",
              "  773367,\n",
              "  772733,\n",
              "  773112,\n",
              "  773113,\n",
              "  772704,\n",
              "  773028,\n",
              "  773453,\n",
              "  773431,\n",
              "  773030,\n",
              "  773160,\n",
              "  773457,\n",
              "  773356,\n",
              "  772728,\n",
              "  773423,\n",
              "  773471,\n",
              "  773024,\n",
              "  773059,\n",
              "  773064,\n",
              "  773074,\n",
              "  773077,\n",
              "  773151,\n",
              "  773467,\n",
              "  773273,\n",
              "  773076,\n",
              "  773279,\n",
              "  773269,\n",
              "  773248,\n",
              "  773472,\n",
              "  85],\n",
              " [773425,\n",
              "  773080,\n",
              "  773459,\n",
              "  772705,\n",
              "  773452,\n",
              "  773202,\n",
              "  773122,\n",
              "  773390,\n",
              "  773141,\n",
              "  773144,\n",
              "  772735,\n",
              "  773161,\n",
              "  773182,\n",
              "  773364,\n",
              "  773340,\n",
              "  773259,\n",
              "  773034,\n",
              "  772990,\n",
              "  772987,\n",
              "  773001,\n",
              "  773234,\n",
              "  772729,\n",
              "  773455,\n",
              "  773196,\n",
              "  773172,\n",
              "  773002,\n",
              "  772988,\n",
              "  773367,\n",
              "  772733,\n",
              "  773112,\n",
              "  773113,\n",
              "  772704,\n",
              "  773028,\n",
              "  773453,\n",
              "  773431,\n",
              "  773030,\n",
              "  773160,\n",
              "  773457,\n",
              "  773356,\n",
              "  772728,\n",
              "  773423,\n",
              "  773471,\n",
              "  773024,\n",
              "  773059,\n",
              "  773064,\n",
              "  773074,\n",
              "  773077,\n",
              "  773151,\n",
              "  773467,\n",
              "  773273,\n",
              "  773076,\n",
              "  773279,\n",
              "  773269,\n",
              "  773248,\n",
              "  773472,\n",
              "  773019,\n",
              "  773029,\n",
              "  773078,\n",
              "  772995,\n",
              "  773329,\n",
              "  773033,\n",
              "  773251,\n",
              "  773343,\n",
              "  773339,\n",
              "  773183,\n",
              "  773184,\n",
              "  70],\n",
              " [773425,\n",
              "  773080,\n",
              "  773459,\n",
              "  772705,\n",
              "  773452,\n",
              "  773202,\n",
              "  773122,\n",
              "  773390,\n",
              "  773141,\n",
              "  773144,\n",
              "  772735,\n",
              "  773161,\n",
              "  773182,\n",
              "  773364,\n",
              "  773340,\n",
              "  773259,\n",
              "  773034,\n",
              "  772990,\n",
              "  772987,\n",
              "  773001,\n",
              "  773234,\n",
              "  772729,\n",
              "  773455,\n",
              "  773196,\n",
              "  773172,\n",
              "  773002,\n",
              "  772988,\n",
              "  773367,\n",
              "  772733,\n",
              "  773112,\n",
              "  773113,\n",
              "  772704,\n",
              "  773028,\n",
              "  773453,\n",
              "  773431,\n",
              "  773030,\n",
              "  773160,\n",
              "  773457,\n",
              "  773356,\n",
              "  772728,\n",
              "  773423,\n",
              "  773471,\n",
              "  773024,\n",
              "  773059,\n",
              "  773064,\n",
              "  773074,\n",
              "  773077,\n",
              "  773151,\n",
              "  773467,\n",
              "  773273,\n",
              "  773076,\n",
              "  773279,\n",
              "  773269,\n",
              "  773248,\n",
              "  773472,\n",
              "  773019,\n",
              "  773029,\n",
              "  773078,\n",
              "  772995,\n",
              "  773329,\n",
              "  773033,\n",
              "  773251,\n",
              "  773343,\n",
              "  773339,\n",
              "  773183,\n",
              "  773184,\n",
              "  773294,\n",
              "  772991,\n",
              "  773114,\n",
              "  773152,\n",
              "  773194,\n",
              "  773187,\n",
              "  772700,\n",
              "  773460,\n",
              "  773021,\n",
              "  773195,\n",
              "  773101,\n",
              "  773387,\n",
              "  773132,\n",
              "  773103,\n",
              "  70],\n",
              " [772990,\n",
              "  772705,\n",
              "  772988,\n",
              "  773122,\n",
              "  773313,\n",
              "  772987,\n",
              "  773172,\n",
              "  773173,\n",
              "  773064,\n",
              "  773164,\n",
              "  773171,\n",
              "  773074,\n",
              "  773144,\n",
              "  773165,\n",
              "  773390,\n",
              "  773178,\n",
              "  773279,\n",
              "  773196,\n",
              "  773269,\n",
              "  773182,\n",
              "  773248,\n",
              "  773234,\n",
              "  773453,\n",
              "  773471,\n",
              "  773187,\n",
              "  773202,\n",
              "  773034,\n",
              "  773080,\n",
              "  773029,\n",
              "  773030,\n",
              "  773024,\n",
              "  773316,\n",
              "  773177,\n",
              "  773273,\n",
              "  773174,\n",
              "  773151,\n",
              "  773076,\n",
              "  70],\n",
              " [772990,\n",
              "  772705,\n",
              "  772988,\n",
              "  773122,\n",
              "  773313,\n",
              "  772987,\n",
              "  773172,\n",
              "  773173,\n",
              "  773064,\n",
              "  773164,\n",
              "  773171,\n",
              "  773074,\n",
              "  773144,\n",
              "  773165,\n",
              "  773390,\n",
              "  773178,\n",
              "  773279,\n",
              "  773196,\n",
              "  773269,\n",
              "  773182,\n",
              "  773248,\n",
              "  773234,\n",
              "  773453,\n",
              "  773471,\n",
              "  773187,\n",
              "  773202,\n",
              "  773034,\n",
              "  773080,\n",
              "  773029,\n",
              "  773030,\n",
              "  773024,\n",
              "  773316,\n",
              "  773177,\n",
              "  773273,\n",
              "  773174,\n",
              "  773151,\n",
              "  773076,\n",
              "  773001,\n",
              "  772729,\n",
              "  773322,\n",
              "  773312,\n",
              "  773077,\n",
              "  772995,\n",
              "  773160,\n",
              "  773114,\n",
              "  772728,\n",
              "  773033,\n",
              "  772700,\n",
              "  773455,\n",
              "  773329,\n",
              "  773343,\n",
              "  773112,\n",
              "  772704,\n",
              "  773356,\n",
              "  773002,\n",
              "  773294,\n",
              "  85],\n",
              " [772990,\n",
              "  772705,\n",
              "  772988,\n",
              "  773122,\n",
              "  773313,\n",
              "  772987,\n",
              "  773172,\n",
              "  773173,\n",
              "  773064,\n",
              "  773164,\n",
              "  773171,\n",
              "  773074,\n",
              "  773144,\n",
              "  773165,\n",
              "  773390,\n",
              "  773178,\n",
              "  773279,\n",
              "  773196,\n",
              "  773269,\n",
              "  773182,\n",
              "  773248,\n",
              "  773234,\n",
              "  773453,\n",
              "  773471,\n",
              "  773187,\n",
              "  773202,\n",
              "  773034,\n",
              "  773080,\n",
              "  773029,\n",
              "  773030,\n",
              "  773024,\n",
              "  773316,\n",
              "  773177,\n",
              "  773273,\n",
              "  773174,\n",
              "  773151,\n",
              "  773076,\n",
              "  773001,\n",
              "  772729,\n",
              "  773322,\n",
              "  773312,\n",
              "  773077,\n",
              "  772995,\n",
              "  773160,\n",
              "  773114,\n",
              "  772728,\n",
              "  773033,\n",
              "  772700,\n",
              "  773455,\n",
              "  773329,\n",
              "  773343,\n",
              "  773112,\n",
              "  772704,\n",
              "  773356,\n",
              "  773002,\n",
              "  773294,\n",
              "  772991,\n",
              "  773128,\n",
              "  773379,\n",
              "  773419,\n",
              "  85],\n",
              " [772990,\n",
              "  772705,\n",
              "  772988,\n",
              "  773122,\n",
              "  773313,\n",
              "  772987,\n",
              "  773172,\n",
              "  773173,\n",
              "  773064,\n",
              "  773164,\n",
              "  773171,\n",
              "  773074,\n",
              "  773144,\n",
              "  773165,\n",
              "  773390,\n",
              "  773178,\n",
              "  773279,\n",
              "  773196,\n",
              "  773269,\n",
              "  773182,\n",
              "  773248,\n",
              "  773234,\n",
              "  773453,\n",
              "  773471,\n",
              "  773187,\n",
              "  773202,\n",
              "  773034,\n",
              "  773080,\n",
              "  773029,\n",
              "  773030,\n",
              "  773024,\n",
              "  773316,\n",
              "  773177,\n",
              "  773273,\n",
              "  773174,\n",
              "  773151,\n",
              "  773076,\n",
              "  773001,\n",
              "  772729,\n",
              "  773322,\n",
              "  773312,\n",
              "  773077,\n",
              "  772995,\n",
              "  773160,\n",
              "  773114,\n",
              "  772728,\n",
              "  773033,\n",
              "  772700,\n",
              "  773455,\n",
              "  773329,\n",
              "  773343,\n",
              "  773112,\n",
              "  772704,\n",
              "  773356,\n",
              "  773002,\n",
              "  773294,\n",
              "  772991,\n",
              "  773128,\n",
              "  773379,\n",
              "  773419,\n",
              "  772997,\n",
              "  70],\n",
              " [772990,\n",
              "  772705,\n",
              "  772988,\n",
              "  773122,\n",
              "  773313,\n",
              "  772987,\n",
              "  773172,\n",
              "  773173,\n",
              "  773064,\n",
              "  773164,\n",
              "  773171,\n",
              "  773074,\n",
              "  773144,\n",
              "  773165,\n",
              "  773390,\n",
              "  773178,\n",
              "  773279,\n",
              "  773196,\n",
              "  773269,\n",
              "  773182,\n",
              "  773248,\n",
              "  773234,\n",
              "  773453,\n",
              "  773471,\n",
              "  773187,\n",
              "  773202,\n",
              "  773034,\n",
              "  773080,\n",
              "  773029,\n",
              "  773030,\n",
              "  773024,\n",
              "  773316,\n",
              "  773177,\n",
              "  773273,\n",
              "  773174,\n",
              "  773151,\n",
              "  773076,\n",
              "  773001,\n",
              "  772729,\n",
              "  773322,\n",
              "  773312,\n",
              "  773077,\n",
              "  772995,\n",
              "  773160,\n",
              "  773114,\n",
              "  772728,\n",
              "  773033,\n",
              "  772700,\n",
              "  773455,\n",
              "  773329,\n",
              "  773343,\n",
              "  773112,\n",
              "  772704,\n",
              "  773356,\n",
              "  773002,\n",
              "  773294,\n",
              "  772991,\n",
              "  773128,\n",
              "  773379,\n",
              "  773419,\n",
              "  772997,\n",
              "  773402,\n",
              "  772994,\n",
              "  773157,\n",
              "  773008,\n",
              "  772734,\n",
              "  773129,\n",
              "  773244,\n",
              "  773345,\n",
              "  70],\n",
              " [772990,\n",
              "  772705,\n",
              "  772988,\n",
              "  773122,\n",
              "  773313,\n",
              "  772987,\n",
              "  773172,\n",
              "  773173,\n",
              "  773064,\n",
              "  773164,\n",
              "  773171,\n",
              "  773074,\n",
              "  773144,\n",
              "  773165,\n",
              "  773390,\n",
              "  773178,\n",
              "  773279,\n",
              "  773196,\n",
              "  773269,\n",
              "  773182,\n",
              "  773248,\n",
              "  773234,\n",
              "  773453,\n",
              "  773471,\n",
              "  773187,\n",
              "  773202,\n",
              "  773034,\n",
              "  773080,\n",
              "  773029,\n",
              "  773030,\n",
              "  773024,\n",
              "  773316,\n",
              "  773177,\n",
              "  773273,\n",
              "  773174,\n",
              "  773151,\n",
              "  773076,\n",
              "  773001,\n",
              "  772729,\n",
              "  773322,\n",
              "  773312,\n",
              "  773077,\n",
              "  772995,\n",
              "  773160,\n",
              "  773114,\n",
              "  772728,\n",
              "  773033,\n",
              "  772700,\n",
              "  773455,\n",
              "  773329,\n",
              "  773343,\n",
              "  773112,\n",
              "  772704,\n",
              "  773356,\n",
              "  773002,\n",
              "  773294,\n",
              "  772991,\n",
              "  773128,\n",
              "  773379,\n",
              "  773419,\n",
              "  772997,\n",
              "  773402,\n",
              "  772994,\n",
              "  773157,\n",
              "  773008,\n",
              "  772734,\n",
              "  773129,\n",
              "  773244,\n",
              "  773345,\n",
              "  772998,\n",
              "  773130,\n",
              "  773346,\n",
              "  85],\n",
              " [772990,\n",
              "  772705,\n",
              "  772988,\n",
              "  773122,\n",
              "  773313,\n",
              "  772987,\n",
              "  773172,\n",
              "  773173,\n",
              "  773064,\n",
              "  773164,\n",
              "  773171,\n",
              "  773074,\n",
              "  773144,\n",
              "  773165,\n",
              "  773390,\n",
              "  773178,\n",
              "  773279,\n",
              "  773196,\n",
              "  773269,\n",
              "  773182,\n",
              "  773248,\n",
              "  773234,\n",
              "  773453,\n",
              "  773471,\n",
              "  773187,\n",
              "  773202,\n",
              "  773034,\n",
              "  773080,\n",
              "  773029,\n",
              "  773030,\n",
              "  773024,\n",
              "  773316,\n",
              "  773177,\n",
              "  773273,\n",
              "  773174,\n",
              "  773151,\n",
              "  773076,\n",
              "  773001,\n",
              "  772729,\n",
              "  773322,\n",
              "  773312,\n",
              "  773077,\n",
              "  772995,\n",
              "  773160,\n",
              "  773114,\n",
              "  772728,\n",
              "  773033,\n",
              "  772700,\n",
              "  773455,\n",
              "  773329,\n",
              "  773343,\n",
              "  773112,\n",
              "  772704,\n",
              "  773356,\n",
              "  773002,\n",
              "  773294,\n",
              "  772991,\n",
              "  773128,\n",
              "  773379,\n",
              "  773419,\n",
              "  772997,\n",
              "  773402,\n",
              "  772994,\n",
              "  773157,\n",
              "  773008,\n",
              "  772734,\n",
              "  773129,\n",
              "  773244,\n",
              "  773345,\n",
              "  772998,\n",
              "  773130,\n",
              "  773346,\n",
              "  772701,\n",
              "  773119,\n",
              "  773115,\n",
              "  772702,\n",
              "  773009,\n",
              "  772698,\n",
              "  773361,\n",
              "  773340,\n",
              "  773099,\n",
              "  772699,\n",
              "  772703,\n",
              "  773161,\n",
              "  773012,\n",
              "  773117,\n",
              "  773364,\n",
              "  773460,\n",
              "  773310,\n",
              "  773259,\n",
              "  772738,\n",
              "  70],\n",
              " [703742,\n",
              "  704228,\n",
              "  703721,\n",
              "  704224,\n",
              "  703894,\n",
              "  704215,\n",
              "  704071,\n",
              "  703943,\n",
              "  703939,\n",
              "  703900,\n",
              "  704227,\n",
              "  703722,\n",
              "  703942,\n",
              "  703732,\n",
              "  703726,\n",
              "  703737,\n",
              "  703723,\n",
              "  704221,\n",
              "  703902,\n",
              "  703741,\n",
              "  704229,\n",
              "  704226,\n",
              "  70],\n",
              " [703742,\n",
              "  704228,\n",
              "  703721,\n",
              "  704224,\n",
              "  703894,\n",
              "  704215,\n",
              "  704071,\n",
              "  703943,\n",
              "  703939,\n",
              "  703900,\n",
              "  704227,\n",
              "  703722,\n",
              "  703942,\n",
              "  703732,\n",
              "  703726,\n",
              "  703737,\n",
              "  703723,\n",
              "  704221,\n",
              "  703902,\n",
              "  703741,\n",
              "  704229,\n",
              "  704226,\n",
              "  703940,\n",
              "  70],\n",
              " [703742,\n",
              "  704228,\n",
              "  703721,\n",
              "  704224,\n",
              "  703894,\n",
              "  704215,\n",
              "  704071,\n",
              "  703943,\n",
              "  703939,\n",
              "  703900,\n",
              "  704227,\n",
              "  703722,\n",
              "  703942,\n",
              "  703732,\n",
              "  703726,\n",
              "  703737,\n",
              "  703723,\n",
              "  704221,\n",
              "  703902,\n",
              "  703741,\n",
              "  704229,\n",
              "  704226,\n",
              "  703940,\n",
              "  704214,\n",
              "  703725,\n",
              "  703729,\n",
              "  704082,\n",
              "  703948,\n",
              "  703895,\n",
              "  85],\n",
              " [703742,\n",
              "  704228,\n",
              "  703721,\n",
              "  704224,\n",
              "  703894,\n",
              "  704215,\n",
              "  704071,\n",
              "  703943,\n",
              "  703939,\n",
              "  703900,\n",
              "  704227,\n",
              "  703722,\n",
              "  703942,\n",
              "  703732,\n",
              "  703726,\n",
              "  703737,\n",
              "  703723,\n",
              "  704221,\n",
              "  703902,\n",
              "  703741,\n",
              "  704229,\n",
              "  704226,\n",
              "  703940,\n",
              "  704214,\n",
              "  703725,\n",
              "  703729,\n",
              "  704082,\n",
              "  703948,\n",
              "  703895,\n",
              "  703901,\n",
              "  704218,\n",
              "  703727,\n",
              "  703735,\n",
              "  704222,\n",
              "  703896,\n",
              "  85],\n",
              " [703742,\n",
              "  704228,\n",
              "  703721,\n",
              "  704224,\n",
              "  703894,\n",
              "  704215,\n",
              "  704071,\n",
              "  703943,\n",
              "  703939,\n",
              "  703900,\n",
              "  704227,\n",
              "  703722,\n",
              "  703942,\n",
              "  703732,\n",
              "  703726,\n",
              "  703737,\n",
              "  703723,\n",
              "  704221,\n",
              "  703902,\n",
              "  703741,\n",
              "  704229,\n",
              "  704226,\n",
              "  703940,\n",
              "  704214,\n",
              "  703725,\n",
              "  703729,\n",
              "  704082,\n",
              "  703948,\n",
              "  703895,\n",
              "  703901,\n",
              "  704218,\n",
              "  703727,\n",
              "  703735,\n",
              "  704222,\n",
              "  703896,\n",
              "  703728,\n",
              "  703897,\n",
              "  85],\n",
              " [703742,\n",
              "  704228,\n",
              "  703721,\n",
              "  704224,\n",
              "  703894,\n",
              "  704215,\n",
              "  704071,\n",
              "  703943,\n",
              "  703939,\n",
              "  703900,\n",
              "  704227,\n",
              "  703722,\n",
              "  703942,\n",
              "  703732,\n",
              "  703726,\n",
              "  703737,\n",
              "  703723,\n",
              "  704221,\n",
              "  703902,\n",
              "  703741,\n",
              "  704229,\n",
              "  704226,\n",
              "  703940,\n",
              "  704214,\n",
              "  703725,\n",
              "  703729,\n",
              "  704082,\n",
              "  703948,\n",
              "  703895,\n",
              "  703901,\n",
              "  704218,\n",
              "  703727,\n",
              "  703735,\n",
              "  704222,\n",
              "  703896,\n",
              "  703728,\n",
              "  703897,\n",
              "  704237,\n",
              "  703898,\n",
              "  85],\n",
              " [703742,\n",
              "  704228,\n",
              "  703721,\n",
              "  704224,\n",
              "  703894,\n",
              "  704215,\n",
              "  704071,\n",
              "  703943,\n",
              "  703939,\n",
              "  703900,\n",
              "  704227,\n",
              "  703722,\n",
              "  703942,\n",
              "  703732,\n",
              "  703726,\n",
              "  703737,\n",
              "  703723,\n",
              "  704221,\n",
              "  703902,\n",
              "  703741,\n",
              "  704229,\n",
              "  704226,\n",
              "  703940,\n",
              "  704214,\n",
              "  703725,\n",
              "  703729,\n",
              "  704082,\n",
              "  703948,\n",
              "  85],\n",
              " [703742,\n",
              "  704228,\n",
              "  703721,\n",
              "  704224,\n",
              "  703894,\n",
              "  704215,\n",
              "  704071,\n",
              "  703943,\n",
              "  703939,\n",
              "  703900,\n",
              "  704227,\n",
              "  703722,\n",
              "  703942,\n",
              "  703732,\n",
              "  703726,\n",
              "  703737,\n",
              "  703723,\n",
              "  704221,\n",
              "  703902,\n",
              "  703741,\n",
              "  704229,\n",
              "  704226,\n",
              "  703940,\n",
              "  704214,\n",
              "  703725,\n",
              "  703729,\n",
              "  704082,\n",
              "  703948,\n",
              "  703895,\n",
              "  703901,\n",
              "  704218,\n",
              "  703727,\n",
              "  703735,\n",
              "  704222,\n",
              "  85],\n",
              " [703742,\n",
              "  704228,\n",
              "  703721,\n",
              "  704224,\n",
              "  703894,\n",
              "  704215,\n",
              "  704071,\n",
              "  703943,\n",
              "  703939,\n",
              "  703900,\n",
              "  704227,\n",
              "  703722,\n",
              "  703942,\n",
              "  703732,\n",
              "  703726,\n",
              "  703737,\n",
              "  703723,\n",
              "  704221,\n",
              "  703902,\n",
              "  703741,\n",
              "  704229,\n",
              "  704226,\n",
              "  703940,\n",
              "  704214,\n",
              "  703725,\n",
              "  703729,\n",
              "  704082,\n",
              "  703948,\n",
              "  703895,\n",
              "  703901,\n",
              "  704218,\n",
              "  703727,\n",
              "  703735,\n",
              "  704222,\n",
              "  703896,\n",
              "  50],\n",
              " [703742,\n",
              "  704228,\n",
              "  703721,\n",
              "  704224,\n",
              "  703894,\n",
              "  704215,\n",
              "  704071,\n",
              "  703943,\n",
              "  703939,\n",
              "  703900,\n",
              "  704227,\n",
              "  703722,\n",
              "  703942,\n",
              "  703732,\n",
              "  703726,\n",
              "  703737,\n",
              "  703723,\n",
              "  704221,\n",
              "  703902,\n",
              "  703741,\n",
              "  704229,\n",
              "  704226,\n",
              "  703940,\n",
              "  704214,\n",
              "  703725,\n",
              "  703729,\n",
              "  704082,\n",
              "  703948,\n",
              "  703895,\n",
              "  703901,\n",
              "  704218,\n",
              "  703727,\n",
              "  703735,\n",
              "  704222,\n",
              "  703896,\n",
              "  703728,\n",
              "  703897,\n",
              "  85],\n",
              " [703742,\n",
              "  704228,\n",
              "  703721,\n",
              "  704224,\n",
              "  703894,\n",
              "  704215,\n",
              "  704071,\n",
              "  703943,\n",
              "  703939,\n",
              "  703900,\n",
              "  704227,\n",
              "  703722,\n",
              "  703942,\n",
              "  703732,\n",
              "  703726,\n",
              "  703737,\n",
              "  703723,\n",
              "  704221,\n",
              "  703902,\n",
              "  703741,\n",
              "  704229,\n",
              "  704226,\n",
              "  703940,\n",
              "  704214,\n",
              "  703725,\n",
              "  703729,\n",
              "  704082,\n",
              "  703948,\n",
              "  703895,\n",
              "  703901,\n",
              "  704218,\n",
              "  703727,\n",
              "  703735,\n",
              "  704222,\n",
              "  703896,\n",
              "  703728,\n",
              "  703897,\n",
              "  704237,\n",
              "  703898,\n",
              "  704240,\n",
              "  703966,\n",
              "  704067,\n",
              "  704125,\n",
              "  704069,\n",
              "  704068,\n",
              "  704066,\n",
              "  704065,\n",
              "  704064,\n",
              "  704063,\n",
              "  703967,\n",
              "  703981,\n",
              "  704062,\n",
              "  85],\n",
              " [768351, 85],\n",
              " [768351, 70],\n",
              " [768351, 50],\n",
              " [768351, 768396, 768472, 768405, 768330, 70],\n",
              " [768351, 768396, 768472, 768405, 768330, 768378, 768379, 768393, 768381, 50],\n",
              " [768351,\n",
              "  768396,\n",
              "  768472,\n",
              "  768405,\n",
              "  768330,\n",
              "  768378,\n",
              "  768379,\n",
              "  768393,\n",
              "  768381,\n",
              "  768586,\n",
              "  70],\n",
              " [768351, 85],\n",
              " [768351, 85],\n",
              " [768351, 70],\n",
              " [768351, 768396, 768472, 768405, 768330, 768378, 768379, 768393, 768381, 85],\n",
              " [768351,\n",
              "  768396,\n",
              "  768472,\n",
              "  768405,\n",
              "  768330,\n",
              "  768378,\n",
              "  768379,\n",
              "  768393,\n",
              "  768381,\n",
              "  768586,\n",
              "  768480,\n",
              "  768380,\n",
              "  768425,\n",
              "  70],\n",
              " [883036,\n",
              "  882654,\n",
              "  882932,\n",
              "  882924,\n",
              "  882923,\n",
              "  882922,\n",
              "  883076,\n",
              "  882537,\n",
              "  882653,\n",
              "  882616,\n",
              "  882626,\n",
              "  882900,\n",
              "  882545,\n",
              "  882940,\n",
              "  883064,\n",
              "  882609,\n",
              "  883142,\n",
              "  883092,\n",
              "  883037,\n",
              "  883289,\n",
              "  883060,\n",
              "  883173,\n",
              "  883143,\n",
              "  883040,\n",
              "  883286,\n",
              "  882602,\n",
              "  883041,\n",
              "  883186,\n",
              "  882617,\n",
              "  883075,\n",
              "  882547,\n",
              "  883288,\n",
              "  883177,\n",
              "  882633,\n",
              "  882701,\n",
              "  882551,\n",
              "  882707,\n",
              "  883148,\n",
              "  882665,\n",
              "  883189,\n",
              "  882638,\n",
              "  882573,\n",
              "  882556,\n",
              "  882554,\n",
              "  882580,\n",
              "  882899,\n",
              "  882921,\n",
              "  883055,\n",
              "  883233,\n",
              "  85],\n",
              " [883036,\n",
              "  882654,\n",
              "  882932,\n",
              "  882924,\n",
              "  882923,\n",
              "  882922,\n",
              "  883076,\n",
              "  882537,\n",
              "  882653,\n",
              "  882616,\n",
              "  882626,\n",
              "  882900,\n",
              "  882545,\n",
              "  882940,\n",
              "  883064,\n",
              "  882609,\n",
              "  883142,\n",
              "  883092,\n",
              "  883037,\n",
              "  883289,\n",
              "  883060,\n",
              "  883173,\n",
              "  883143,\n",
              "  883040,\n",
              "  883286,\n",
              "  882602,\n",
              "  883041,\n",
              "  883186,\n",
              "  882617,\n",
              "  883075,\n",
              "  882547,\n",
              "  883288,\n",
              "  883177,\n",
              "  882633,\n",
              "  882701,\n",
              "  882551,\n",
              "  882707,\n",
              "  883148,\n",
              "  882665,\n",
              "  883189,\n",
              "  882638,\n",
              "  882573,\n",
              "  882556,\n",
              "  882554,\n",
              "  882580,\n",
              "  882899,\n",
              "  882921,\n",
              "  883055,\n",
              "  883233,\n",
              "  882674,\n",
              "  883042,\n",
              "  883043,\n",
              "  882722,\n",
              "  882577,\n",
              "  882723,\n",
              "  883081,\n",
              "  882724,\n",
              "  882581,\n",
              "  883089,\n",
              "  882971,\n",
              "  882587,\n",
              "  883247,\n",
              "  85],\n",
              " [883036,\n",
              "  882654,\n",
              "  882932,\n",
              "  882924,\n",
              "  882923,\n",
              "  882922,\n",
              "  883076,\n",
              "  882537,\n",
              "  882653,\n",
              "  882616,\n",
              "  882626,\n",
              "  882900,\n",
              "  882545,\n",
              "  882940,\n",
              "  883064,\n",
              "  882609,\n",
              "  883142,\n",
              "  883092,\n",
              "  883037,\n",
              "  883289,\n",
              "  883060,\n",
              "  883173,\n",
              "  883143,\n",
              "  883040,\n",
              "  883286,\n",
              "  882602,\n",
              "  883041,\n",
              "  883186,\n",
              "  882617,\n",
              "  883075,\n",
              "  882547,\n",
              "  883288,\n",
              "  883177,\n",
              "  882633,\n",
              "  882701,\n",
              "  882551,\n",
              "  882707,\n",
              "  883148,\n",
              "  882665,\n",
              "  883189,\n",
              "  882638,\n",
              "  882573,\n",
              "  882556,\n",
              "  882554,\n",
              "  882580,\n",
              "  882899,\n",
              "  882921,\n",
              "  883055,\n",
              "  883233,\n",
              "  882674,\n",
              "  883042,\n",
              "  883043,\n",
              "  882722,\n",
              "  882577,\n",
              "  882723,\n",
              "  883081,\n",
              "  882724,\n",
              "  882581,\n",
              "  883089,\n",
              "  882971,\n",
              "  882587,\n",
              "  883247,\n",
              "  882618,\n",
              "  882675,\n",
              "  882975,\n",
              "  882610,\n",
              "  882672,\n",
              "  883088,\n",
              "  882646,\n",
              "  882676,\n",
              "  883045,\n",
              "  882658,\n",
              "  882647,\n",
              "  882645,\n",
              "  883259,\n",
              "  883084,\n",
              "  882566,\n",
              "  882562,\n",
              "  882668,\n",
              "  882561,\n",
              "  882671,\n",
              "  882659,\n",
              "  882650,\n",
              "  882947,\n",
              "  882563,\n",
              "  883047,\n",
              "  882656,\n",
              "  882996,\n",
              "  882625,\n",
              "  882926,\n",
              "  883023,\n",
              "  70],\n",
              " [883036,\n",
              "  882654,\n",
              "  882932,\n",
              "  882924,\n",
              "  882923,\n",
              "  882922,\n",
              "  883076,\n",
              "  882537,\n",
              "  882653,\n",
              "  882616,\n",
              "  882626,\n",
              "  882900,\n",
              "  882545,\n",
              "  882940,\n",
              "  883064,\n",
              "  882609,\n",
              "  883142,\n",
              "  883092,\n",
              "  883037,\n",
              "  883289,\n",
              "  883060,\n",
              "  883173,\n",
              "  883143,\n",
              "  883040,\n",
              "  883286,\n",
              "  882602,\n",
              "  883041,\n",
              "  883186,\n",
              "  882617,\n",
              "  883075,\n",
              "  882547,\n",
              "  883288,\n",
              "  883177,\n",
              "  882633,\n",
              "  882701,\n",
              "  882551,\n",
              "  882707,\n",
              "  883148,\n",
              "  882665,\n",
              "  883189,\n",
              "  882638,\n",
              "  882573,\n",
              "  882556,\n",
              "  882554,\n",
              "  882580,\n",
              "  882899,\n",
              "  882921,\n",
              "  883055,\n",
              "  883233,\n",
              "  882674,\n",
              "  883042,\n",
              "  883043,\n",
              "  882722,\n",
              "  882577,\n",
              "  882723,\n",
              "  883081,\n",
              "  882724,\n",
              "  882581,\n",
              "  883089,\n",
              "  882971,\n",
              "  882587,\n",
              "  883247,\n",
              "  882618,\n",
              "  882675,\n",
              "  882975,\n",
              "  882610,\n",
              "  882672,\n",
              "  883088,\n",
              "  882646,\n",
              "  882676,\n",
              "  883045,\n",
              "  882658,\n",
              "  882647,\n",
              "  882645,\n",
              "  883259,\n",
              "  883084,\n",
              "  882566,\n",
              "  882562,\n",
              "  882668,\n",
              "  882561,\n",
              "  882671,\n",
              "  882659,\n",
              "  882650,\n",
              "  882947,\n",
              "  882563,\n",
              "  883047,\n",
              "  882656,\n",
              "  882996,\n",
              "  882625,\n",
              "  882926,\n",
              "  883023,\n",
              "  882673,\n",
              "  882592,\n",
              "  883049,\n",
              "  883278,\n",
              "  883277,\n",
              "  882550,\n",
              "  882705,\n",
              "  882709,\n",
              "  882684,\n",
              "  882596,\n",
              "  882558,\n",
              "  937371,\n",
              "  882600,\n",
              "  883029,\n",
              "  941543,\n",
              "  883085,\n",
              "  882655,\n",
              "  882657,\n",
              "  882590,\n",
              "  883048,\n",
              "  882598,\n",
              "  882630,\n",
              "  882652,\n",
              "  882564,\n",
              "  882677,\n",
              "  85],\n",
              " [883036,\n",
              "  882654,\n",
              "  882932,\n",
              "  882924,\n",
              "  882923,\n",
              "  882922,\n",
              "  883076,\n",
              "  882537,\n",
              "  882653,\n",
              "  882616,\n",
              "  882626,\n",
              "  882900,\n",
              "  882545,\n",
              "  882940,\n",
              "  883064,\n",
              "  882609,\n",
              "  883142,\n",
              "  883092,\n",
              "  883037,\n",
              "  883289,\n",
              "  883060,\n",
              "  883173,\n",
              "  883143,\n",
              "  883040,\n",
              "  883286,\n",
              "  882602,\n",
              "  883041,\n",
              "  883186,\n",
              "  882617,\n",
              "  883075,\n",
              "  882547,\n",
              "  883288,\n",
              "  883177,\n",
              "  882633,\n",
              "  882701,\n",
              "  882551,\n",
              "  882707,\n",
              "  883148,\n",
              "  882665,\n",
              "  883189,\n",
              "  882638,\n",
              "  882573,\n",
              "  882556,\n",
              "  882554,\n",
              "  882580,\n",
              "  882899,\n",
              "  882921,\n",
              "  883055,\n",
              "  883233,\n",
              "  882674,\n",
              "  883042,\n",
              "  883043,\n",
              "  882722,\n",
              "  882577,\n",
              "  882723,\n",
              "  883081,\n",
              "  882724,\n",
              "  882581,\n",
              "  883089,\n",
              "  882971,\n",
              "  882587,\n",
              "  883247,\n",
              "  882618,\n",
              "  882675,\n",
              "  882975,\n",
              "  882610,\n",
              "  882672,\n",
              "  883088,\n",
              "  882646,\n",
              "  882676,\n",
              "  883045,\n",
              "  882658,\n",
              "  882647,\n",
              "  882645,\n",
              "  883259,\n",
              "  883084,\n",
              "  882566,\n",
              "  882562,\n",
              "  882668,\n",
              "  882561,\n",
              "  882671,\n",
              "  882659,\n",
              "  882650,\n",
              "  882947,\n",
              "  882563,\n",
              "  883047,\n",
              "  882656,\n",
              "  882996,\n",
              "  882625,\n",
              "  882926,\n",
              "  883023,\n",
              "  882673,\n",
              "  882592,\n",
              "  883049,\n",
              "  883278,\n",
              "  883277,\n",
              "  882550,\n",
              "  882705,\n",
              "  882709,\n",
              "  882684,\n",
              "  882596,\n",
              "  882558,\n",
              "  937371,\n",
              "  882600,\n",
              "  883029,\n",
              "  941543,\n",
              "  883085,\n",
              "  882655,\n",
              "  882657,\n",
              "  882590,\n",
              "  883048,\n",
              "  882598,\n",
              "  882630,\n",
              "  882652,\n",
              "  882564,\n",
              "  882677,\n",
              "  882629,\n",
              "  882608,\n",
              "  883169,\n",
              "  883046,\n",
              "  883073,\n",
              "  882711,\n",
              "  882685,\n",
              "  882559,\n",
              "  882611,\n",
              "  882713,\n",
              "  882712,\n",
              "  882721,\n",
              "  883301,\n",
              "  882584,\n",
              "  883271,\n",
              "  882679,\n",
              "  883027,\n",
              "  882565,\n",
              "  883302,\n",
              "  882620,\n",
              "  85],\n",
              " [883036,\n",
              "  882654,\n",
              "  882932,\n",
              "  882924,\n",
              "  882923,\n",
              "  882922,\n",
              "  883076,\n",
              "  882537,\n",
              "  882653,\n",
              "  882616,\n",
              "  882626,\n",
              "  882900,\n",
              "  882545,\n",
              "  882940,\n",
              "  883064,\n",
              "  882609,\n",
              "  883142,\n",
              "  883092,\n",
              "  883037,\n",
              "  883289,\n",
              "  883060,\n",
              "  883173,\n",
              "  883143,\n",
              "  883040,\n",
              "  883286,\n",
              "  882602,\n",
              "  883041,\n",
              "  883186,\n",
              "  882617,\n",
              "  883075,\n",
              "  882547,\n",
              "  883288,\n",
              "  883177,\n",
              "  882633,\n",
              "  882701,\n",
              "  882551,\n",
              "  882707,\n",
              "  883148,\n",
              "  882665,\n",
              "  883189,\n",
              "  882638,\n",
              "  882573,\n",
              "  882556,\n",
              "  882554,\n",
              "  882580,\n",
              "  882899,\n",
              "  882921,\n",
              "  883055,\n",
              "  883233,\n",
              "  882674,\n",
              "  883042,\n",
              "  883043,\n",
              "  882722,\n",
              "  882577,\n",
              "  882723,\n",
              "  883081,\n",
              "  882724,\n",
              "  882581,\n",
              "  883089,\n",
              "  882971,\n",
              "  882587,\n",
              "  883247,\n",
              "  882618,\n",
              "  882675,\n",
              "  85],\n",
              " [883036,\n",
              "  882654,\n",
              "  882932,\n",
              "  882924,\n",
              "  882923,\n",
              "  882922,\n",
              "  883076,\n",
              "  882537,\n",
              "  882653,\n",
              "  882616,\n",
              "  882626,\n",
              "  882900,\n",
              "  882545,\n",
              "  882940,\n",
              "  883064,\n",
              "  882609,\n",
              "  883142,\n",
              "  883092,\n",
              "  883037,\n",
              "  883289,\n",
              "  883060,\n",
              "  883173,\n",
              "  883143,\n",
              "  883040,\n",
              "  883286,\n",
              "  882602,\n",
              "  883041,\n",
              "  883186,\n",
              "  882617,\n",
              "  883075,\n",
              "  882547,\n",
              "  883288,\n",
              "  883177,\n",
              "  882633,\n",
              "  882701,\n",
              "  882551,\n",
              "  882707,\n",
              "  883148,\n",
              "  882665,\n",
              "  883189,\n",
              "  882638,\n",
              "  882573,\n",
              "  882556,\n",
              "  882554,\n",
              "  882580,\n",
              "  882899,\n",
              "  882921,\n",
              "  883055,\n",
              "  883233,\n",
              "  882674,\n",
              "  883042,\n",
              "  883043,\n",
              "  882722,\n",
              "  882577,\n",
              "  882723,\n",
              "  883081,\n",
              "  882724,\n",
              "  882581,\n",
              "  883089,\n",
              "  882971,\n",
              "  882587,\n",
              "  883247,\n",
              "  882618,\n",
              "  882675,\n",
              "  882975,\n",
              "  882610,\n",
              "  882672,\n",
              "  883088,\n",
              "  882646,\n",
              "  85],\n",
              " [883036,\n",
              "  882654,\n",
              "  882932,\n",
              "  882924,\n",
              "  882923,\n",
              "  882922,\n",
              "  883076,\n",
              "  882537,\n",
              "  882653,\n",
              "  882616,\n",
              "  882626,\n",
              "  882900,\n",
              "  882545,\n",
              "  882940,\n",
              "  883064,\n",
              "  882609,\n",
              "  883142,\n",
              "  883092,\n",
              "  883037,\n",
              "  883289,\n",
              "  883060,\n",
              "  883173,\n",
              "  883143,\n",
              "  883040,\n",
              "  883286,\n",
              "  882602,\n",
              "  883041,\n",
              "  883186,\n",
              "  882617,\n",
              "  883075,\n",
              "  882547,\n",
              "  883288,\n",
              "  883177,\n",
              "  882633,\n",
              "  882701,\n",
              "  882551,\n",
              "  882707,\n",
              "  883148,\n",
              "  882665,\n",
              "  883189,\n",
              "  882638,\n",
              "  882573,\n",
              "  882556,\n",
              "  882554,\n",
              "  882580,\n",
              "  882899,\n",
              "  882921,\n",
              "  883055,\n",
              "  883233,\n",
              "  882674,\n",
              "  883042,\n",
              "  883043,\n",
              "  882722,\n",
              "  882577,\n",
              "  882723,\n",
              "  883081,\n",
              "  882724,\n",
              "  882581,\n",
              "  883089,\n",
              "  882971,\n",
              "  882587,\n",
              "  883247,\n",
              "  882618,\n",
              "  882675,\n",
              "  882975,\n",
              "  882610,\n",
              "  882672,\n",
              "  883088,\n",
              "  882646,\n",
              "  882676,\n",
              "  883045,\n",
              "  882658,\n",
              "  882647,\n",
              "  882645,\n",
              "  883259,\n",
              "  883084,\n",
              "  882566,\n",
              "  882562,\n",
              "  882668,\n",
              "  882561,\n",
              "  882671,\n",
              "  882659,\n",
              "  882650,\n",
              "  882947,\n",
              "  882563,\n",
              "  883047,\n",
              "  882656,\n",
              "  882996,\n",
              "  882625,\n",
              "  882926,\n",
              "  883023,\n",
              "  882673,\n",
              "  882592,\n",
              "  883049,\n",
              "  70],\n",
              " [883036,\n",
              "  882654,\n",
              "  882932,\n",
              "  882924,\n",
              "  882923,\n",
              "  882922,\n",
              "  883076,\n",
              "  882537,\n",
              "  882653,\n",
              "  882616,\n",
              "  882626,\n",
              "  882900,\n",
              "  882545,\n",
              "  882940,\n",
              "  883064,\n",
              "  882609,\n",
              "  883142,\n",
              "  883092,\n",
              "  883037,\n",
              "  883289,\n",
              "  883060,\n",
              "  883173,\n",
              "  883143,\n",
              "  883040,\n",
              "  883286,\n",
              "  882602,\n",
              "  883041,\n",
              "  883186,\n",
              "  882617,\n",
              "  883075,\n",
              "  882547,\n",
              "  883288,\n",
              "  883177,\n",
              "  882633,\n",
              "  882701,\n",
              "  882551,\n",
              "  882707,\n",
              "  883148,\n",
              "  882665,\n",
              "  883189,\n",
              "  882638,\n",
              "  882573,\n",
              "  882556,\n",
              "  882554,\n",
              "  882580,\n",
              "  882899,\n",
              "  882921,\n",
              "  883055,\n",
              "  883233,\n",
              "  882674,\n",
              "  883042,\n",
              "  883043,\n",
              "  882722,\n",
              "  882577,\n",
              "  882723,\n",
              "  883081,\n",
              "  882724,\n",
              "  882581,\n",
              "  883089,\n",
              "  882971,\n",
              "  882587,\n",
              "  883247,\n",
              "  882618,\n",
              "  882675,\n",
              "  882975,\n",
              "  882610,\n",
              "  882672,\n",
              "  883088,\n",
              "  882646,\n",
              "  882676,\n",
              "  883045,\n",
              "  882658,\n",
              "  882647,\n",
              "  882645,\n",
              "  883259,\n",
              "  883084,\n",
              "  882566,\n",
              "  882562,\n",
              "  882668,\n",
              "  882561,\n",
              "  882671,\n",
              "  882659,\n",
              "  882650,\n",
              "  882947,\n",
              "  882563,\n",
              "  883047,\n",
              "  882656,\n",
              "  882996,\n",
              "  882625,\n",
              "  882926,\n",
              "  883023,\n",
              "  882673,\n",
              "  882592,\n",
              "  883049,\n",
              "  883278,\n",
              "  883277,\n",
              "  882550,\n",
              "  882705,\n",
              "  882709,\n",
              "  882684,\n",
              "  882596,\n",
              "  882558,\n",
              "  937371,\n",
              "  882600,\n",
              "  883029,\n",
              "  941543,\n",
              "  85],\n",
              " [883036,\n",
              "  882654,\n",
              "  882932,\n",
              "  882924,\n",
              "  882923,\n",
              "  882922,\n",
              "  883076,\n",
              "  882537,\n",
              "  882653,\n",
              "  882616,\n",
              "  882626,\n",
              "  882900,\n",
              "  882545,\n",
              "  882940,\n",
              "  883064,\n",
              "  882609,\n",
              "  883142,\n",
              "  883092,\n",
              "  883037,\n",
              "  883289,\n",
              "  883060,\n",
              "  883173,\n",
              "  883143,\n",
              "  883040,\n",
              "  883286,\n",
              "  882602,\n",
              "  883041,\n",
              "  883186,\n",
              "  882617,\n",
              "  883075,\n",
              "  882547,\n",
              "  883288,\n",
              "  883177,\n",
              "  882633,\n",
              "  882701,\n",
              "  882551,\n",
              "  882707,\n",
              "  883148,\n",
              "  882665,\n",
              "  883189,\n",
              "  882638,\n",
              "  882573,\n",
              "  882556,\n",
              "  882554,\n",
              "  882580,\n",
              "  882899,\n",
              "  882921,\n",
              "  883055,\n",
              "  883233,\n",
              "  882674,\n",
              "  883042,\n",
              "  883043,\n",
              "  882722,\n",
              "  882577,\n",
              "  882723,\n",
              "  883081,\n",
              "  882724,\n",
              "  882581,\n",
              "  883089,\n",
              "  882971,\n",
              "  882587,\n",
              "  883247,\n",
              "  882618,\n",
              "  882675,\n",
              "  882975,\n",
              "  882610,\n",
              "  882672,\n",
              "  883088,\n",
              "  882646,\n",
              "  882676,\n",
              "  883045,\n",
              "  882658,\n",
              "  882647,\n",
              "  882645,\n",
              "  883259,\n",
              "  883084,\n",
              "  882566,\n",
              "  882562,\n",
              "  882668,\n",
              "  882561,\n",
              "  882671,\n",
              "  882659,\n",
              "  882650,\n",
              "  882947,\n",
              "  882563,\n",
              "  883047,\n",
              "  882656,\n",
              "  882996,\n",
              "  882625,\n",
              "  882926,\n",
              "  883023,\n",
              "  882673,\n",
              "  882592,\n",
              "  883049,\n",
              "  883278,\n",
              "  883277,\n",
              "  882550,\n",
              "  882705,\n",
              "  882709,\n",
              "  882684,\n",
              "  882596,\n",
              "  882558,\n",
              "  937371,\n",
              "  882600,\n",
              "  883029,\n",
              "  941543,\n",
              "  883085,\n",
              "  882655,\n",
              "  882657,\n",
              "  882590,\n",
              "  883048,\n",
              "  882598,\n",
              "  882630,\n",
              "  882652,\n",
              "  882564,\n",
              "  882677,\n",
              "  882629,\n",
              "  882608,\n",
              "  883169,\n",
              "  883046,\n",
              "  883073,\n",
              "  882711,\n",
              "  882685,\n",
              "  882559,\n",
              "  882611,\n",
              "  882713,\n",
              "  882712,\n",
              "  882721,\n",
              "  883301,\n",
              "  882584,\n",
              "  883271,\n",
              "  882679,\n",
              "  883027,\n",
              "  70],\n",
              " [883036,\n",
              "  882654,\n",
              "  882932,\n",
              "  882924,\n",
              "  882923,\n",
              "  882922,\n",
              "  883076,\n",
              "  882537,\n",
              "  882653,\n",
              "  882616,\n",
              "  882626,\n",
              "  882900,\n",
              "  882545,\n",
              "  882940,\n",
              "  883064,\n",
              "  882609,\n",
              "  883142,\n",
              "  883092,\n",
              "  883037,\n",
              "  883289,\n",
              "  883060,\n",
              "  883173,\n",
              "  883143,\n",
              "  883040,\n",
              "  883286,\n",
              "  882602,\n",
              "  883041,\n",
              "  883186,\n",
              "  882617,\n",
              "  883075,\n",
              "  882547,\n",
              "  883288,\n",
              "  883177,\n",
              "  882633,\n",
              "  882701,\n",
              "  882551,\n",
              "  882707,\n",
              "  883148,\n",
              "  882665,\n",
              "  883189,\n",
              "  882638,\n",
              "  882573,\n",
              "  882556,\n",
              "  882554,\n",
              "  882580,\n",
              "  882899,\n",
              "  882921,\n",
              "  883055,\n",
              "  883233,\n",
              "  882674,\n",
              "  883042,\n",
              "  883043,\n",
              "  882722,\n",
              "  882577,\n",
              "  882723,\n",
              "  883081,\n",
              "  882724,\n",
              "  882581,\n",
              "  883089,\n",
              "  882971,\n",
              "  882587,\n",
              "  883247,\n",
              "  882618,\n",
              "  882675,\n",
              "  882975,\n",
              "  882610,\n",
              "  882672,\n",
              "  883088,\n",
              "  882646,\n",
              "  882676,\n",
              "  883045,\n",
              "  882658,\n",
              "  882647,\n",
              "  882645,\n",
              "  883259,\n",
              "  883084,\n",
              "  882566,\n",
              "  882562,\n",
              "  882668,\n",
              "  882561,\n",
              "  882671,\n",
              "  882659,\n",
              "  882650,\n",
              "  882947,\n",
              "  882563,\n",
              "  883047,\n",
              "  882656,\n",
              "  882996,\n",
              "  882625,\n",
              "  882926,\n",
              "  883023,\n",
              "  882673,\n",
              "  882592,\n",
              "  883049,\n",
              "  883278,\n",
              "  883277,\n",
              "  882550,\n",
              "  882705,\n",
              "  882709,\n",
              "  882684,\n",
              "  882596,\n",
              "  882558,\n",
              "  937371,\n",
              "  882600,\n",
              "  883029,\n",
              "  941543,\n",
              "  883085,\n",
              "  882655,\n",
              "  882657,\n",
              "  882590,\n",
              "  883048,\n",
              "  882598,\n",
              "  882630,\n",
              "  882652,\n",
              "  882564,\n",
              "  882677,\n",
              "  882629,\n",
              "  882608,\n",
              "  883169,\n",
              "  883046,\n",
              "  883073,\n",
              "  882711,\n",
              "  882685,\n",
              "  882559,\n",
              "  882611,\n",
              "  882713,\n",
              "  882712,\n",
              "  882721,\n",
              "  883301,\n",
              "  882584,\n",
              "  883271,\n",
              "  882679,\n",
              "  883027,\n",
              "  882565,\n",
              "  883302,\n",
              "  882620,\n",
              "  973777,\n",
              "  883058,\n",
              "  883091,\n",
              "  882622,\n",
              "  882683,\n",
              "  882937,\n",
              "  882682,\n",
              "  882635,\n",
              "  882571,\n",
              "  70],\n",
              " [883036,\n",
              "  882654,\n",
              "  882932,\n",
              "  882924,\n",
              "  882923,\n",
              "  882922,\n",
              "  883076,\n",
              "  882537,\n",
              "  882653,\n",
              "  882616,\n",
              "  882626,\n",
              "  882900,\n",
              "  882545,\n",
              "  882940,\n",
              "  883064,\n",
              "  882609,\n",
              "  883142,\n",
              "  883092,\n",
              "  883037,\n",
              "  883289,\n",
              "  883060,\n",
              "  883173,\n",
              "  883143,\n",
              "  883040,\n",
              "  883286,\n",
              "  882602,\n",
              "  883041,\n",
              "  883186,\n",
              "  882617,\n",
              "  883075,\n",
              "  882547,\n",
              "  883288,\n",
              "  883177,\n",
              "  882633,\n",
              "  882701,\n",
              "  882551,\n",
              "  882707,\n",
              "  883148,\n",
              "  882665,\n",
              "  883189,\n",
              "  882638,\n",
              "  882573,\n",
              "  882556,\n",
              "  882554,\n",
              "  882580,\n",
              "  882899,\n",
              "  882921,\n",
              "  883055,\n",
              "  883233,\n",
              "  882674,\n",
              "  883042,\n",
              "  883043,\n",
              "  882722,\n",
              "  882577,\n",
              "  882723,\n",
              "  883081,\n",
              "  882724,\n",
              "  882581,\n",
              "  883089,\n",
              "  882971,\n",
              "  882587,\n",
              "  883247,\n",
              "  882618,\n",
              "  882675,\n",
              "  882975,\n",
              "  882610,\n",
              "  882672,\n",
              "  883088,\n",
              "  882646,\n",
              "  882676,\n",
              "  883045,\n",
              "  882658,\n",
              "  882647,\n",
              "  882645,\n",
              "  883259,\n",
              "  883084,\n",
              "  882566,\n",
              "  882562,\n",
              "  882668,\n",
              "  882561,\n",
              "  882671,\n",
              "  882659,\n",
              "  882650,\n",
              "  882947,\n",
              "  882563,\n",
              "  883047,\n",
              "  882656,\n",
              "  882996,\n",
              "  882625,\n",
              "  882926,\n",
              "  883023,\n",
              "  882673,\n",
              "  882592,\n",
              "  883049,\n",
              "  883278,\n",
              "  883277,\n",
              "  882550,\n",
              "  882705,\n",
              "  882709,\n",
              "  882684,\n",
              "  882596,\n",
              "  882558,\n",
              "  937371,\n",
              "  882600,\n",
              "  883029,\n",
              "  941543,\n",
              "  883085,\n",
              "  882655,\n",
              "  882657,\n",
              "  882590,\n",
              "  883048,\n",
              "  882598,\n",
              "  882630,\n",
              "  882652,\n",
              "  882564,\n",
              "  882677,\n",
              "  882629,\n",
              "  882608,\n",
              "  883169,\n",
              "  883046,\n",
              "  883073,\n",
              "  882711,\n",
              "  882685,\n",
              "  882559,\n",
              "  882611,\n",
              "  882713,\n",
              "  882712,\n",
              "  882721,\n",
              "  883301,\n",
              "  882584,\n",
              "  883271,\n",
              "  882679,\n",
              "  883027,\n",
              "  882565,\n",
              "  883302,\n",
              "  882620,\n",
              "  973777,\n",
              "  883058,\n",
              "  883091,\n",
              "  882622,\n",
              "  882683,\n",
              "  882937,\n",
              "  85],\n",
              " [729671,\n",
              "  729789,\n",
              "  730057,\n",
              "  729787,\n",
              "  730035,\n",
              "  730007,\n",
              "  730012,\n",
              "  730050,\n",
              "  730135,\n",
              "  730068,\n",
              "  730065,\n",
              "  85],\n",
              " [729671,\n",
              "  729789,\n",
              "  730057,\n",
              "  729787,\n",
              "  730035,\n",
              "  730007,\n",
              "  730012,\n",
              "  730050,\n",
              "  730135,\n",
              "  730068,\n",
              "  730065,\n",
              "  730054,\n",
              "  730059,\n",
              "  729779,\n",
              "  730053,\n",
              "  730071,\n",
              "  730075,\n",
              "  730017,\n",
              "  70],\n",
              " [729671,\n",
              "  729789,\n",
              "  730057,\n",
              "  729787,\n",
              "  730035,\n",
              "  730007,\n",
              "  730012,\n",
              "  730050,\n",
              "  730135,\n",
              "  730068,\n",
              "  730065,\n",
              "  730054,\n",
              "  730059,\n",
              "  729779,\n",
              "  730053,\n",
              "  730071,\n",
              "  730075,\n",
              "  730017,\n",
              "  730130,\n",
              "  729682,\n",
              "  729788,\n",
              "  729683,\n",
              "  730116,\n",
              "  730091,\n",
              "  50],\n",
              " [729671, 729789, 730057, 729787, 730035, 730007, 730012, 50],\n",
              " [729671,\n",
              "  729789,\n",
              "  730057,\n",
              "  729787,\n",
              "  730035,\n",
              "  730007,\n",
              "  730012,\n",
              "  730050,\n",
              "  730135,\n",
              "  730068,\n",
              "  730065,\n",
              "  730054,\n",
              "  730059,\n",
              "  729779,\n",
              "  730053,\n",
              "  730071,\n",
              "  85],\n",
              " [729671,\n",
              "  729789,\n",
              "  730057,\n",
              "  729787,\n",
              "  730035,\n",
              "  730007,\n",
              "  730012,\n",
              "  730050,\n",
              "  730135,\n",
              "  730068,\n",
              "  730065,\n",
              "  730054,\n",
              "  730059,\n",
              "  729779,\n",
              "  730053,\n",
              "  730071,\n",
              "  730075,\n",
              "  730017,\n",
              "  730130,\n",
              "  729682,\n",
              "  729788,\n",
              "  729683,\n",
              "  0],\n",
              " [729671,\n",
              "  729789,\n",
              "  730057,\n",
              "  729787,\n",
              "  730035,\n",
              "  730007,\n",
              "  730012,\n",
              "  730050,\n",
              "  730135,\n",
              "  730068,\n",
              "  730065,\n",
              "  730054,\n",
              "  730059,\n",
              "  729779,\n",
              "  730053,\n",
              "  730071,\n",
              "  730075,\n",
              "  730017,\n",
              "  730130,\n",
              "  729682,\n",
              "  729788,\n",
              "  729683,\n",
              "  730116,\n",
              "  730091,\n",
              "  730014,\n",
              "  730118,\n",
              "  730031,\n",
              "  729785,\n",
              "  730117,\n",
              "  730094,\n",
              "  730093,\n",
              "  0],\n",
              " [729671,\n",
              "  729789,\n",
              "  730057,\n",
              "  729787,\n",
              "  730035,\n",
              "  730007,\n",
              "  730012,\n",
              "  730050,\n",
              "  730135,\n",
              "  730068,\n",
              "  730065,\n",
              "  730054,\n",
              "  730059,\n",
              "  729779,\n",
              "  730053,\n",
              "  730071,\n",
              "  730075,\n",
              "  730017,\n",
              "  730130,\n",
              "  729682,\n",
              "  729788,\n",
              "  729683,\n",
              "  730116,\n",
              "  730091,\n",
              "  730014,\n",
              "  730118,\n",
              "  730031,\n",
              "  729785,\n",
              "  730117,\n",
              "  730094,\n",
              "  730093,\n",
              "  730088,\n",
              "  730127,\n",
              "  730018,\n",
              "  730006,\n",
              "  730016,\n",
              "  729796,\n",
              "  730110,\n",
              "  729795,\n",
              "  70],\n",
              " [779619,\n",
              "  779089,\n",
              "  779623,\n",
              "  779612,\n",
              "  779675,\n",
              "  779335,\n",
              "  779405,\n",
              "  779332,\n",
              "  779620,\n",
              "  779461,\n",
              "  779452,\n",
              "  779374,\n",
              "  779451,\n",
              "  779379,\n",
              "  779384,\n",
              "  779380,\n",
              "  779427,\n",
              "  779652,\n",
              "  779439,\n",
              "  779653,\n",
              "  779671,\n",
              "  779075,\n",
              "  779647,\n",
              "  779713,\n",
              "  779833,\n",
              "  779556,\n",
              "  779259,\n",
              "  779628,\n",
              "  779342,\n",
              "  779351,\n",
              "  779349,\n",
              "  779726,\n",
              "  779387,\n",
              "  779625,\n",
              "  779391,\n",
              "  779672,\n",
              "  779447,\n",
              "  85],\n",
              " [779619,\n",
              "  779089,\n",
              "  779623,\n",
              "  779612,\n",
              "  779675,\n",
              "  779335,\n",
              "  779405,\n",
              "  779332,\n",
              "  779620,\n",
              "  779461,\n",
              "  779452,\n",
              "  779374,\n",
              "  779451,\n",
              "  779379,\n",
              "  779384,\n",
              "  779380,\n",
              "  779427,\n",
              "  779652,\n",
              "  779439,\n",
              "  779653,\n",
              "  779671,\n",
              "  779075,\n",
              "  779647,\n",
              "  779713,\n",
              "  779833,\n",
              "  779556,\n",
              "  779259,\n",
              "  779628,\n",
              "  779342,\n",
              "  779351,\n",
              "  779349,\n",
              "  779726,\n",
              "  779387,\n",
              "  779625,\n",
              "  779391,\n",
              "  779672,\n",
              "  779447,\n",
              "  779527,\n",
              "  779490,\n",
              "  779431,\n",
              "  779449,\n",
              "  779668,\n",
              "  779727,\n",
              "  779728,\n",
              "  779624,\n",
              "  779078,\n",
              "  779076,\n",
              "  779441,\n",
              "  779347,\n",
              "  779725,\n",
              "  779260,\n",
              "  779437,\n",
              "  779546,\n",
              "  779331,\n",
              "  779651,\n",
              "  779276,\n",
              "  779385,\n",
              "  70],\n",
              " [779619,\n",
              "  779089,\n",
              "  779623,\n",
              "  779612,\n",
              "  779675,\n",
              "  779335,\n",
              "  779405,\n",
              "  779332,\n",
              "  779620,\n",
              "  779461,\n",
              "  779452,\n",
              "  779374,\n",
              "  779451,\n",
              "  779379,\n",
              "  779384,\n",
              "  779380,\n",
              "  779427,\n",
              "  779652,\n",
              "  779439,\n",
              "  779653,\n",
              "  779671,\n",
              "  779075,\n",
              "  779647,\n",
              "  779713,\n",
              "  779833,\n",
              "  779556,\n",
              "  779259,\n",
              "  779628,\n",
              "  779342,\n",
              "  779351,\n",
              "  779349,\n",
              "  779726,\n",
              "  779387,\n",
              "  779625,\n",
              "  779391,\n",
              "  779672,\n",
              "  779447,\n",
              "  779527,\n",
              "  779490,\n",
              "  779431,\n",
              "  779449,\n",
              "  779668,\n",
              "  779727,\n",
              "  779728,\n",
              "  779624,\n",
              "  779078,\n",
              "  779076,\n",
              "  779441,\n",
              "  779347,\n",
              "  779725,\n",
              "  779260,\n",
              "  779437,\n",
              "  779546,\n",
              "  779331,\n",
              "  779651,\n",
              "  779276,\n",
              "  779385,\n",
              "  779682,\n",
              "  779080,\n",
              "  779429,\n",
              "  779355,\n",
              "  779394,\n",
              "  779363,\n",
              "  779629,\n",
              "  779358,\n",
              "  779336,\n",
              "  779395,\n",
              "  779261,\n",
              "  779696,\n",
              "  779401,\n",
              "  779400,\n",
              "  779630,\n",
              "  779700,\n",
              "  779402,\n",
              "  779631,\n",
              "  779567,\n",
              "  779522,\n",
              "  779568,\n",
              "  779352,\n",
              "  779361,\n",
              "  779371,\n",
              "  779362,\n",
              "  85],\n",
              " [779619,\n",
              "  779089,\n",
              "  779623,\n",
              "  779612,\n",
              "  779675,\n",
              "  779335,\n",
              "  779405,\n",
              "  779332,\n",
              "  779620,\n",
              "  779461,\n",
              "  779452,\n",
              "  779374,\n",
              "  779451,\n",
              "  779379,\n",
              "  779384,\n",
              "  779380,\n",
              "  779427,\n",
              "  779652,\n",
              "  779439,\n",
              "  779653,\n",
              "  779671,\n",
              "  779075,\n",
              "  779647,\n",
              "  779713,\n",
              "  779833,\n",
              "  779556,\n",
              "  779259,\n",
              "  779628,\n",
              "  779342,\n",
              "  779351,\n",
              "  779349,\n",
              "  779726,\n",
              "  779387,\n",
              "  85],\n",
              " [779619,\n",
              "  779089,\n",
              "  779623,\n",
              "  779612,\n",
              "  779675,\n",
              "  779335,\n",
              "  779405,\n",
              "  779332,\n",
              "  779620,\n",
              "  779461,\n",
              "  779452,\n",
              "  779374,\n",
              "  779451,\n",
              "  779379,\n",
              "  779384,\n",
              "  779380,\n",
              "  779427,\n",
              "  779652,\n",
              "  779439,\n",
              "  779653,\n",
              "  779671,\n",
              "  779075,\n",
              "  779647,\n",
              "  779713,\n",
              "  779833,\n",
              "  779556,\n",
              "  779259,\n",
              "  779628,\n",
              "  779342,\n",
              "  779351,\n",
              "  779349,\n",
              "  779726,\n",
              "  779387,\n",
              "  779625,\n",
              "  779391,\n",
              "  779672,\n",
              "  779447,\n",
              "  779527,\n",
              "  779490,\n",
              "  779431,\n",
              "  779449,\n",
              "  779668,\n",
              "  779727,\n",
              "  779728,\n",
              "  779624,\n",
              "  779078,\n",
              "  779076,\n",
              "  779441,\n",
              "  779347,\n",
              "  779725,\n",
              "  779260,\n",
              "  779437,\n",
              "  779546,\n",
              "  779331,\n",
              "  779651,\n",
              "  779276,\n",
              "  779385,\n",
              "  779682,\n",
              "  779080,\n",
              "  779429,\n",
              "  70],\n",
              " [779619,\n",
              "  779089,\n",
              "  779623,\n",
              "  779612,\n",
              "  779675,\n",
              "  779335,\n",
              "  779405,\n",
              "  779332,\n",
              "  779620,\n",
              "  779461,\n",
              "  779452,\n",
              "  779374,\n",
              "  779451,\n",
              "  779379,\n",
              "  779384,\n",
              "  779380,\n",
              "  779427,\n",
              "  779652,\n",
              "  779439,\n",
              "  779653,\n",
              "  779671,\n",
              "  779075,\n",
              "  779647,\n",
              "  779713,\n",
              "  779833,\n",
              "  779556,\n",
              "  779259,\n",
              "  779628,\n",
              "  779342,\n",
              "  779351,\n",
              "  779349,\n",
              "  779726,\n",
              "  779387,\n",
              "  779625,\n",
              "  779391,\n",
              "  779672,\n",
              "  779447,\n",
              "  779527,\n",
              "  779490,\n",
              "  779431,\n",
              "  779449,\n",
              "  779668,\n",
              "  779727,\n",
              "  779728,\n",
              "  779624,\n",
              "  779078,\n",
              "  779076,\n",
              "  779441,\n",
              "  779347,\n",
              "  779725,\n",
              "  779260,\n",
              "  779437,\n",
              "  779546,\n",
              "  779331,\n",
              "  779651,\n",
              "  779276,\n",
              "  779385,\n",
              "  779682,\n",
              "  779080,\n",
              "  779429,\n",
              "  779355,\n",
              "  779394,\n",
              "  779363,\n",
              "  779629,\n",
              "  779358,\n",
              "  779336,\n",
              "  779395,\n",
              "  779261,\n",
              "  779696,\n",
              "  779401,\n",
              "  779400,\n",
              "  779630,\n",
              "  779700,\n",
              "  779402,\n",
              "  779631,\n",
              "  779567,\n",
              "  779522,\n",
              "  779568,\n",
              "  779352,\n",
              "  779361,\n",
              "  779371,\n",
              "  779362,\n",
              "  779343,\n",
              "  779614,\n",
              "  779613,\n",
              "  779369,\n",
              "  70]]"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def score_to_grade_bucket(score):\n",
        "    \"\"\"\n",
        "    Converts a score to a grade bucket\n",
        "    \"\"\"\n",
        "    if score >= 85:\n",
        "        return 85\n",
        "    elif score >= 70:\n",
        "        return 70\n",
        "    elif score >= 50:\n",
        "        return 50\n",
        "    else:\n",
        "        return 0\n",
        "\n",
        "def get_resource_sets_per_student(student_id):\n",
        "    \"\"\"\n",
        "    Returns the set of resources a student has interacted with before an assessment.\n",
        "    \"\"\"\n",
        "    global student_vle_df\n",
        "    global student_assessment_df\n",
        "    student_vle_interaction = get_vle_interaction_by_student(student_id)\n",
        "    student_assessment = student_assessment_df[student_assessment_df['id_student'] == student_id]\n",
        "    student_vle_sets = []\n",
        "    for index, assessment in student_assessment.iterrows():\n",
        "        assessment_date = assessment[\"date_submitted\"]\n",
        "        student_vle_set = student_vle_interaction[student_vle_interaction[\"date\"] < assessment_date] # ?  Say there are 3 assessments, we are saying that resources used before assessment 1 also affect the outcome of assessment 3\n",
        "        student_vle_set : np.ndarray = student_vle_set[\"id_site\"].unique()\n",
        "        student_vle_set = student_vle_set.tolist()\n",
        "        student_vle_set.append(score_to_grade_bucket(assessment[\"score\"])) # ? Should I use the weighted Score?\n",
        "        student_vle_sets.append(student_vle_set)\n",
        "    return student_vle_sets\n",
        "\n",
        "def get_resource_sets(sample_size = None):\n",
        "    \"\"\"\n",
        "    Generates and returns the resource sets for all students\n",
        "    \"\"\"\n",
        "    global student_vle_df\n",
        "    global student_assessment_df\n",
        "\n",
        "    resource_sets = []\n",
        "\n",
        "    SAMPLE_SIZE = sample_size if sample_size is not None else student_info_df.shape[0]\n",
        "    for index, student in tqdm(student_info_df.sample(n=SAMPLE_SIZE, random_state=1).iterrows(), total=SAMPLE_SIZE, desc=\"Processing Students\"):\n",
        "        student_resource_set = get_resource_sets_per_student(student[\"id_student\"])\n",
        "        resource_sets.extend(student_resource_set)\n",
        "    return resource_sets\n",
        "\n",
        "def write_sets_to_file(file_name: str, resource_sets):\n",
        "    with open(file_name, 'w') as f:\n",
        "        for item in resource_sets:\n",
        "            for i in item:\n",
        "                f.write(\"%s \" % i)\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "def get_resource_sets_from_file(file_name):\n",
        "    \"\"\"\n",
        "    Reads the resource sets from a file\n",
        "    \"\"\"\n",
        "    with open(file_name, 'r') as f:\n",
        "        resource_sets = []\n",
        "        for line in f:\n",
        "            resource_set = line.split(\" \")\n",
        "            resource_set.pop(-1)\n",
        "            resource_set = [int(resource) for resource in resource_set]\n",
        "            resource_sets.append(resource_set)\n",
        "        return resource_sets\n",
        "\n",
        "\n",
        "# get_resource_sets_per_student(student_info_df.sample(n=1, random_state=1).iloc[0][\"id_student\"])\n",
        "get_resource_sets(sample_size=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VpgYO2n37l7b"
      },
      "source": [
        "Observation:\n",
        "\n",
        "Currently I get itemsets such that a resource is considered for all assessments after it. This results in really long transaction sets. This results in really long and a lot of itemsets. This also skews support metrics in favor of resources that are perhaps used early on in the course. This is not semantically correct."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "Ub7oU8x17l7b"
      },
      "outputs": [],
      "source": [
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "from mlxtend.frequent_patterns import fpgrowth\n",
        "from mlxtend.frequent_patterns import fpmax, association_rules\n",
        "\"\"\"\n",
        "    Association Analysis using mlxtend and FP Growth\n",
        "\"\"\"\n",
        "\n",
        "# ! This is conventional association analysis. We need to encode for sequential mining\n",
        "\n",
        "def generate_rules(resource_sets):\n",
        "    print(len(resource_sets))\n",
        "    te = TransactionEncoder()\n",
        "    te.fit(resource_sets)\n",
        "    resource_sets_encoded = te.transform(resource_sets)\n",
        "    df = pd.DataFrame(resource_sets_encoded, columns=te.columns_)\n",
        "    df.head()\n",
        "\n",
        "    # fpgrowth finds all frequent itemsets\n",
        "    frequent_itemsets = fpgrowth(df, min_support=0.6, use_colnames=True)\n",
        "\n",
        "    # fpmax finds only maximal frequent itemsets\n",
        "    maximal_itemsets = fpmax(df, min_support=0.10, use_colnames=True)\n",
        "\n",
        "    rules = association_rules(frequent_itemsets, num_itemsets = frequent_itemsets.shape[0],metric=\"confidence\", min_threshold=0.7)\n",
        "    return rules\n",
        "\n",
        "\n",
        "# TODO Filter itemsets by score\n",
        "# TODO print out the supports for each of the items\n",
        "# TODO Account for skewed support distribution\n",
        "# TODO see if you can generate closed\n",
        "# TODO See if a boolean value for 'if' the score went up\n",
        "\n",
        "# with open('data/resource_sets.txt', 'w') as f:\n",
        "    # for item in resource_sets:\n",
        "        # for i in item:\n",
        "            # f.write(\"%s \" % i)\n",
        "        # f.write(\"\\n\")\n",
        "    # f.flush()\n",
        "# generate_rules(resource_sets)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dq-u9hui7l7b"
      },
      "source": [
        "### Sequential Analysis\n",
        "---\n",
        "TODO Sequential algorithms sometimes consider A, B, C and A, B as different. So try turnign the flag off"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "fF3RsH9K7l7c"
      },
      "outputs": [],
      "source": [
        "# from spmf import Spmf\n",
        "#\n",
        "# sp = Spmf(\"PrefixSpan\", input_direct=\"data/resource_sets.txt\", output_filename=\"output.txt\", arguments=[0.1], spmf_bin_location_dir=\"data\")\n",
        "# sp.run()\n",
        "# results = sp.to_pandas_dataframe()\n",
        "# print(results)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "RM5k8Yw_7l7c"
      },
      "outputs": [],
      "source": [
        "# from prefixspan import PrefixSpan\n",
        "#\n",
        "# ps = PrefixSpan(resource_sets)\n",
        "# ps.frequent(10, closed=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XYPcfr37l7c"
      },
      "source": [
        "## Neural Netowrks\n",
        "\n",
        "We are going to try sequential mining with Neural Networks. RNNs, LSTMs, and then transformers. These models have memory states that would be useful for remembering sequential data and capturing patterns in resources that lead to good results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "LlI2BGo37l7c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-04-22 23:05:44.710843: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745381144.719406    7103 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745381144.721865    7103 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "W0000 00:00:1745381144.729630    7103 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1745381144.729640    7103 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1745381144.729643    7103 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "W0000 00:00:1745381144.729646    7103 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "2025-04-22 23:05:44.732850: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import seaborn as sns\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, LSTM, Reshape\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from mlxtend.preprocessing import TransactionEncoder\n",
        "# from keras.preprocessing.text import Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSdaxnyX7l7c"
      },
      "source": [
        "### Preprocessing: LSTMs\n",
        "---\n",
        "\n",
        "- A 'Timestep' is one session before a test. We are only considering 50 resources.\n",
        "- <b>Question</b> This leads to the issue of ignoring early resources in the grand scheme of things. However, I am going off examples of LSTMs in recommender systems where each recommendation system uses items of the same semantic significance. The difference in our case is that Scores and IDs are separate. I am going to try both."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "Ayf18lXL7l7c",
        "outputId": "f38df738-0ade-4638-a8d8-625d37cd043e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Prepare data for LSTM\n",
        "\n",
        "# resources_sets = get_resource_sets()\n",
        "resources_sets = get_resource_sets_from_file(\"data/resource_sets.txt\")\n",
        "print(len(resources_sets))\n",
        "\n",
        "MAX_LEN = 10\n",
        "\n",
        "# plot the number of resources in each set\n",
        "def plot_resource_set_lengths(sets):\n",
        "    lengths = []\n",
        "    for resource_set in sets:\n",
        "        lengths.append(len(resource_set))\n",
        "    sns.scatterplot(x=range(len(lengths)), y=lengths)\n",
        "\n",
        "plot_resource_set_lengths(resources_sets)\n",
        "\n",
        "def pad_resource_sets(resource_sets):\n",
        "    \"\"\"\n",
        "    Pads the resource sets\n",
        "    \"\"\"\n",
        "    return pad_sequences(resource_sets, padding='pre', truncating='pre', maxlen=MAX_LEN)\n",
        "\n",
        "def tokenize_resource_sets(resource_sets):\n",
        "    \"\"\"\n",
        "    Tokenizes the resource sets\n",
        "    \"\"\"\n",
        "    pass \n",
        "\n",
        "def get_sets_labels(padded_resource_sets):\n",
        "    \"\"\"\n",
        "    Returns the sets and labels\n",
        "    \"\"\"\n",
        "    sets = padded_resource_sets[:, :-1]\n",
        "    labels = padded_resource_sets[:, -1]\n",
        "    return sets, labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_filtered_student_resources(student_id):\n",
        "    student_resource_set = get_resource_sets_per_student(student_id)\n",
        "    \n",
        "    index = 0 \n",
        "    while (index < len(student_resource_set)):\n",
        "        # remove any transaction with the final result less than 85\n",
        "        if (student_resource_set[index][-1] < 85):\n",
        "            student_resource_set.pop(index)\n",
        "        else:\n",
        "            index += 1\n",
        "    \n",
        "     \n",
        "    return student_resource_set \n",
        "\n",
        "def get_resource_df(student_vle_df, student_info_df, vle_df, sample_size=100):\n",
        "    \"\"\"\n",
        "        Create a Dataframe for training the LSTM. The Dataframe should contains\n",
        "        a sequence of resources used by sucessful students before a test with a successful outcome.\n",
        "    \"\"\"\n",
        "    successful_students_df = get_successful_student_trends(student_info_df.sample(n=sample_size))\n",
        "\n",
        "    unique_vle_resources = vle_df[\"id_site\"].unique()\n",
        "    vocab_size = vle_df[\"id_site\"].unique().shape[0]\n",
        "    \n",
        "    # Create a mapping from resource id to index\n",
        "    resource_mapping = {}\n",
        "    inverse_resource_mapping = {}\n",
        "    for index, resource in enumerate(unique_vle_resources):\n",
        "        resource_mapping[resource] = index\n",
        "        inverse_resource_mapping[index] = resource\n",
        "\n",
        "    # Add a mapping for the padding index    \n",
        "    resource_mapping[0] = 0\n",
        "    inverse_resource_mapping[0] = 0\n",
        "\n",
        "    vectorized_map = np.vectorize(resource_mapping.get)\n",
        "\n",
        "\n",
        "    vle_df = pd.get_dummies(vle_df, columns=[\"code_module\", \"code_presentation\"])\n",
        "    student_info_df = student_info_df.drop(columns=[\"code_module\", \"code_presentation\"])\n",
        "    resource_sequences = []\n",
        "    # For each student fetch resources used before a test filter \n",
        "    for index, student in successful_students_df.iterrows():\n",
        "        student_id = student[\"id_student\"]\n",
        "        student_resource_set = get_filtered_student_resources(student_id) \n",
        "        if (len(student_resource_set) == 0):\n",
        "            continue\n",
        "\n",
        "        padded_student_resource_set = pad_resource_sets(student_resource_set) \n",
        "        # Convert the resource set to a sequence of indices\n",
        "        \n",
        "        mapped_resource_sets = []\n",
        "        for list_resources in padded_student_resource_set:\n",
        "            resource_set = vectorized_map(list_resources[:-1]) \n",
        "            resource_set_encoded = to_categorical(resource_set, num_classes=vocab_size)\n",
        "            temp_resource_set_encoded = []\n",
        "\n",
        "            # get resource features from the vle based on resource id in list_resources\n",
        "            for index, resource in enumerate(resource_set[:-1]):\n",
        "                if inverse_resource_mapping[resource] != 0 and inverse_resource_mapping[resource]  not in unique_vle_resources:\n",
        "                    print(inverse_resource_mapping[resource])\n",
        "                    print(\"Resource not found in vle_df\") \n",
        "                    continue\n",
        "                \n",
        "                if (inverse_resource_mapping[resource] == 0):\n",
        "                    # append a vector of zeros\n",
        "                    resource_encoded = np.zeros(vocab_size + vle_df.shape[1] + student_info_df.shape[1] - 2) # minus 2 for the id_site and student_id columns\n",
        "                    temp_resource_set_encoded.append(resource_encoded)\n",
        "                    continue\n",
        "                \n",
        "                resource_df = vle_df[vle_df['id_site'] == inverse_resource_mapping[resource]]\n",
        "                student_df = student_info_df[student_info_df['id_student'] == student_id] \n",
        "\n",
        "                # We can add richer information based on student scores to these resources \n",
        "                # However, we have already filtered these sequences to only include successful students\n",
        "\n",
        "                # append resource_df and student_df to resource in temp_resource_set_encoded\n",
        "                resource_encoded = resource_set_encoded[index].tolist()\n",
        "                resource_encoded.extend(resource_df.iloc[0, 1:].values)\n",
        "                resource_encoded.extend(student_df.iloc[0, 1:].values)\n",
        "                temp_resource_set_encoded.append(np.array(resource_encoded))\n",
        "\n",
        "            resource_set = np.array(temp_resource_set_encoded)\n",
        "            if len(resource_set) == 0:\n",
        "                continue \n",
        "\n",
        "            mapped_resource_sets.append(resource_set)\n",
        "        resource_sequences.extend(mapped_resource_sets)\n",
        "    \n",
        "\n",
        "    # separate the features and labels\n",
        "    resource_sequences = np.array(resource_sequences)\n",
        "    x = resource_sequences[:, :-1]\n",
        "    y = resource_sequences[:, -1]\n",
        "\n",
        "    # strip all resources in y of the student and resource features\n",
        "    y = y[:, :vocab_size]\n",
        "     \n",
        "    return x, y\n",
        "\n",
        "# get_resource_df(student_vle_df, student_info_df, vle_df)\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Students: 100%|██████████| 1000/1000 [00:01<00:00, 839.64it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(560, 7, 6421)\n",
            "(560, 6364)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/tanay24/cs370/cs370/venv/lib/python3.10/site-packages/keras/src/layers/core/wrapper.py:27: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "I0000 00:00:1745381168.340331    7103 gpu_device.cc:2019] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6258 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
            "2025-04-22 23:06:08.561987: W external/local_xla/xla/service/gpu/llvm_gpu_backend/default/nvptx_libdevice_path.cc:40] Can't find libdevice directory ${CUDA_DIR}/nvvm/libdevice. This may result in compilation or runtime failures, if the program we try to run uses routines from libdevice.\n",
            "Searched for CUDA in the following directories:\n",
            "  ./cuda_sdk_lib\n",
            "  ipykernel_launcher.runfiles/cuda_nvcc\n",
            "  ipykern/cuda_nvcc\n",
            "  \n",
            "  /usr/local/cuda\n",
            "  /opt/cuda\n",
            "  /home/tanay24/cs370/cs370/venv/lib/python3.10/site-packages/tensorflow/python/platform/../../../nvidia/cuda_nvcc\n",
            "  /home/tanay24/cs370/cs370/venv/lib/python3.10/site-packages/tensorflow/python/platform/../../../../nvidia/cuda_nvcc\n",
            "  /home/tanay24/cs370/cs370/venv/lib/python3.10/site-packages/tensorflow/python/platform/../../cuda\n",
            "  /home/tanay24/cs370/cs370/venv/lib/python3.10/site-packages/tensorflow/python/platform/../../../../../..\n",
            "  /home/tanay24/cs370/cs370/venv/lib/python3.10/site-packages/tensorflow/python/platform/../../../../../../..\n",
            "  .\n",
            "You can choose the search directory by setting xla_gpu_cuda_data_dir in HloModule's DebugOptions.  For most apps, setting the environment variable XLA_FLAGS=--xla_gpu_cuda_data_dir=/path/to/cuda will work.\n",
            "2025-04-22 23:06:08.567950: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:187] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
            "2025-04-22 23:06:08.569813: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:187] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
            "2025-04-22 23:06:08.571516: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:187] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
            "2025-04-22 23:06:08.573219: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:187] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
            "2025-04-22 23:06:08.575427: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:187] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
            "2025-04-22 23:06:08.577378: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:187] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
            "2025-04-22 23:06:08.579005: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:187] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
            "2025-04-22 23:06:08.581083: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:187] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
            "2025-04-22 23:06:08.582749: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:187] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
            "2025-04-22 23:06:08.584451: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:187] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
            "2025-04-22 23:06:08.586044: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:187] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
            "2025-04-22 23:06:08.587513: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:187] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
            "2025-04-22 23:06:08.589811: W tensorflow/compiler/mlir/tools/kernel_gen/transforms/gpu_kernel_to_blob_pass.cc:187] Failed to compile generated PTX with ptxas. Falling back to compilation by driver.\n",
            "2025-04-22 23:06:08.852947: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleLoadData(&module, data)' failed with 'CUDA_ERROR_UNSUPPORTED_PTX_VERSION'\n",
            "\n",
            "2025-04-22 23:06:08.852966: W tensorflow/compiler/mlir/tools/kernel_gen/tf_gpu_runtime_wrappers.cc:40] 'cuModuleGetFunction(&function, module, kernel_name)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
            "\n",
            "2025-04-22 23:06:08.852973: W tensorflow/core/framework/op_kernel.cc:1844] INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n",
            "2025-04-22 23:06:08.852989: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: INTERNAL: 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE'\n"
          ]
        },
        {
          "ename": "InternalError",
          "evalue": "{{function_node __wrapped__FloorMod_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:FloorMod] name: ",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mInternalError\u001b[0m                             Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[76], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m Sequential()\n\u001b[0;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mTimeDistributed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6421\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# TODO Try adding an Embedding layer and removing the other features\u001b[39;00m\n\u001b[1;32m     15\u001b[0m model\u001b[38;5;241m.\u001b[39madd(LSTM(\u001b[38;5;241m128\u001b[39m))\n\u001b[1;32m     16\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dropout(\u001b[38;5;241m0.2\u001b[39m))\n",
            "File \u001b[0;32m~/cs370/cs370/venv/lib/python3.10/site-packages/keras/src/models/sequential.py:122\u001b[0m, in \u001b[0;36mSequential.add\u001b[0;34m(self, layer, rebuild)\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers\u001b[38;5;241m.\u001b[39mappend(layer)\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rebuild:\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_rebuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuilt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
            "File \u001b[0;32m~/cs370/cs370/venv/lib/python3.10/site-packages/keras/src/models/sequential.py:149\u001b[0m, in \u001b[0;36mSequential._maybe_rebuild\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m], InputLayer) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    148\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mbatch_shape\n\u001b[0;32m--> 149\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_shape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;66;03m# We can build the Sequential model if the first layer has the\u001b[39;00m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;66;03m# `input_shape` property. This is most commonly found in Functional\u001b[39;00m\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;66;03m# model.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m     input_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39minput_shape\n",
            "File \u001b[0;32m~/cs370/cs370/venv/lib/python3.10/site-packages/keras/src/layers/layer.py:230\u001b[0m, in \u001b[0;36mLayer.__new__.<locals>.build_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_open_name_scope():\n\u001b[1;32m    229\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_path \u001b[38;5;241m=\u001b[39m current_path()\n\u001b[0;32m--> 230\u001b[0m     \u001b[43moriginal_build_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;66;03m# Record build config.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m signature \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39msignature(original_build_method)\n",
            "File \u001b[0;32m~/cs370/cs370/venv/lib/python3.10/site-packages/keras/src/models/sequential.py:195\u001b[0m, in \u001b[0;36mSequential.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_layers[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 195\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m:\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;66;03m# Can happen if shape inference is not implemented.\u001b[39;00m\n\u001b[1;32m    198\u001b[0m         \u001b[38;5;66;03m# TODO: consider reverting inbound nodes on layers processed.\u001b[39;00m\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "File \u001b[0;32m~/cs370/cs370/venv/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[0;32m~/cs370/cs370/venv/lib/python3.10/site-packages/keras/src/backend/tensorflow/random.py:19\u001b[0m, in \u001b[0;36m_cast_seed\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m seed\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m     seed \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloormod\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mint32\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m seed\n",
            "\u001b[0;31mInternalError\u001b[0m: {{function_node __wrapped__FloorMod_device_/job:localhost/replica:0/task:0/device:GPU:0}} 'cuLaunchKernel(function, gridX, gridY, gridZ, blockX, blockY, blockZ, 0, reinterpret_cast<CUstream>(stream), params, nullptr)' failed with 'CUDA_ERROR_INVALID_HANDLE' [Op:FloorMod] name: "
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Embedding, Dropout, Dense, LSTM, Reshape, TimeDistributed, Input\n",
        "from keras.optimizers import Adam\n",
        "from time import time\n",
        "\n",
        "gpu_devices = tf.config.list_physical_devices('GPU')\n",
        "print(gpu_devices)\n",
        "\n",
        "x, y = get_resource_df(student_vle_df, student_info_df, vle_df, sample_size=1000)\n",
        "print(x.shape)\n",
        "print(y.shape)\n",
        "model = Sequential()\n",
        "model.add(TimeDistributed(Dense(256, activation='relu'), input_shape=(7, 6421))) # TODO Try adding an Embedding layer and removing the other features\n",
        "model.add(LSTM(128))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(6364, activation='sigmoid'))\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy', 'mae'])\n",
        "\n",
        "h = model.fit(x, y, epochs=60, batch_size=32, validation_split=0.2)\n",
        "plt.figure()\n",
        "plt.plot(h.history['loss'], label='Train loss')\n",
        "plt.plot(h.history['val_loss'], label='Validation loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "# save the plot as a png file named as loss_datetime.png\n",
        "plt.savefig(f\"graphs/loss_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.png\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(h.history['accuracy'], label='Train accuracy')\n",
        "plt.plot(h.history['val_accuracy'], label='Validation accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.savefig(f\"graphs/accuracy_{datetime.datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing Students: 100%|██████████| 5000/5000 [00:05<00:00, 888.86it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2322, 7, 6364])\n",
            "torch.Size([2322, 7, 57])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Hyperparams\n",
        "num_resources = 6364\n",
        "meta_dim = 57\n",
        "embed_dim = 128\n",
        "meta_out_dim = 32\n",
        "lstm_hidden_dim = 128\n",
        "seq_len = 7\n",
        "\n",
        "x, y = get_resource_df(student_vle_df, student_info_df, vle_df, sample_size=5000)\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "x = torch.tensor(x, dtype=torch.float32)  # (batch, seq, num_resources + meta_dim)\n",
        "y = torch.tensor(y, dtype=torch.float32)  # (batch, num_resources)\n",
        "\n",
        "# Split input into resource IDs and metadata\n",
        "resource_onehot = x[:, :, :num_resources]       # (batch, seq, 6436)\n",
        "print(resource_onehot.shape)\n",
        "metadata = x[:, :, num_resources:]              # (batch, seq, 57)\n",
        "print(metadata.shape)\n",
        "resource_ids = torch.argmax(resource_onehot, dim=-1)  # (batch, seq)\n",
        "\n",
        "# Define layers\n",
        "resource_embedding = nn.Embedding(num_resources, embed_dim)\n",
        "metadata_dense = nn.Linear(meta_dim, meta_out_dim)\n",
        "lstm = nn.LSTM(input_size=embed_dim + meta_out_dim, hidden_size=lstm_hidden_dim, batch_first=True)\n",
        "output_layer = nn.Linear(lstm_hidden_dim, num_resources)\n",
        "\n",
        "# Forward pass\n",
        "resource_embed = resource_embedding(resource_ids)                   # (batch, seq, embed_dim)\n",
        "metadata_embed = F.relu(metadata_dense(metadata))                  # (batch, seq, meta_out_dim)\n",
        "lstm_input = torch.cat([resource_embed, metadata_embed], dim=-1)   # (batch, seq, embed + meta)\n",
        "\n",
        "lstm_out, _ = lstm(lstm_input)             # (batch, seq, hidden_dim)\n",
        "final_output = lstm_out[:, -1, :]          # (batch, hidden_dim)\n",
        "pred = torch.sigmoid(output_layer(final_output))  # (batch, 6436)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[79], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(pred, y)\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, loss\u001b[38;5;241m.\u001b[39mitem())\n",
            "File \u001b[0;32m~/cs370/cs370/venv/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/cs370/cs370/venv/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/cs370/cs370/venv/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
          ]
        }
      ],
      "source": [
        "# Assume y is (batch, 6436)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam([\n",
        "    *resource_embedding.parameters(),\n",
        "    *metadata_dense.parameters(),\n",
        "    *lstm.parameters(),\n",
        "    *output_layer.parameters()\n",
        "], lr=0.001)\n",
        "\n",
        "loss = criterion(pred, y)\n",
        "optimizer.zero_grad()\n",
        "loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(\"Loss:\", loss.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Class Implementation of a pytorch LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing\n",
        "\n",
        "- Hold out a test \n",
        "- Find a better Resource features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ4n8rpD7l7d"
      },
      "source": [
        "### Compiling LSTM Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eevtNKUp7l7d",
        "outputId": "532daf02-1cd1-400c-d3c5-aec63f45c965"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[41 39 37 35 40 18 16  7 15]\n",
            " [ 9 11 41 39 40 18 16  7 15]\n",
            " [10 41 39 35 40 18 16  7 15]\n",
            " [13  6  8 17  9 39 18  7 42]\n",
            " [10 41 39 35 40 18 16  7 15]\n",
            " [ 6  8 38 17  9 39 40 18  7]\n",
            " [10 41 39 35 40 18 16  7 15]\n",
            " [12  4 36 13  8 17  9  7 42]\n",
            " [10 41 39 35 40 18 16  7 15]\n",
            " [ 5 44 14 12 36 13  8 17  7]\n",
            " [34 14 12 36 13  8 17 44  7]\n",
            " [10 41 39 35 40 18 16  7 15]\n",
            " [41 39 37 35 40 18 16  7 15]\n",
            " [10 41 39 35 40 18 16  7 15]\n",
            " [39 37 35 40 43 18 16  7 15]\n",
            " [13  6  8 17  9 39 40 18  7]\n",
            " [10 41 39 35 40 18 16  7 15]\n",
            " [ 6  8 38 17  9 39 40 18  7]\n",
            " [49 52 62 61 55 51 53 58 59]\n",
            " [60 48 58 59 46 47 57 56 54]\n",
            " [60 48 58 59 46 47 57 56 54]\n",
            " [60 48 58 59 46 47 57 56 54]\n",
            " [51 50 53 45 63 60 58 59 54]\n",
            " [25 32 24 23 21 20 27 30 22]\n",
            " [33 19 31 26 28 32 21 29 30]\n",
            " [28 25 32 24 23 21 20 30 22]]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1745357601.089855    7363 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
            "W0000 00:00:1745357601.090802    7363 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(26, 9)\n",
            "(26, 4)\n",
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - accuracy: 0.6000 - loss: 1.3850 - val_accuracy: 0.3636 - val_loss: 1.3850\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.4667 - loss: 1.3733 - val_accuracy: 0.2727 - val_loss: 1.3838\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.4667 - loss: 1.3608 - val_accuracy: 0.2727 - val_loss: 1.3826\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.4667 - loss: 1.3467 - val_accuracy: 0.2727 - val_loss: 1.3815\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step - accuracy: 0.4667 - loss: 1.3304 - val_accuracy: 0.2727 - val_loss: 1.3806\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step - accuracy: 0.4667 - loss: 1.3114 - val_accuracy: 0.2727 - val_loss: 1.3799\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 55ms/step - accuracy: 0.4667 - loss: 1.2889 - val_accuracy: 0.2727 - val_loss: 1.3796\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.4667 - loss: 1.2625 - val_accuracy: 0.2727 - val_loss: 1.3799\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step - accuracy: 0.4667 - loss: 1.2313 - val_accuracy: 0.2727 - val_loss: 1.3813\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 0.4667 - loss: 1.1949 - val_accuracy: 0.2727 - val_loss: 1.3844\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7576acc21ea0>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "te = TransactionEncoder()\n",
        "padded_resource_sets = pad_resource_sets(resources_sets)\n",
        "\n",
        "# TODO this seems impossible\n",
        "\n",
        "# Todo for any itemset (eg. A, B, C) find all the students that have this itemset and check their labels\n",
        "\n",
        "padded_sequence, labels = get_sets_labels(padded_resource_sets)\n",
        "\n",
        "te.fit(padded_sequence)\n",
        "padded_sequence = te.transform(padded_sequence)\n",
        "\n",
        "unique_ids = np.unique(padded_resource_sets.flatten())\n",
        "\n",
        "# print(unique_ids.shape)\n",
        "\n",
        "id_to_index = {id: index for index, id in enumerate(unique_ids)}\n",
        "index_to_id = {index: id for index, id in enumerate(unique_ids)}\n",
        "\n",
        "\n",
        "vectorized_map = np.vectorize(id_to_index.get)\n",
        "tokenized_resource_sequence = vectorized_map(padded_resource_sets)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "labels = label_encoder.fit_transform(labels)\n",
        "labels = to_categorical(labels, num_classes=np.unique(labels).shape[0])\n",
        "\n",
        "\n",
        "# drop last element from every list in tokenized_resource_sequence\n",
        "tokenized_resource_sequence = np.delete(tokenized_resource_sequence, -1, axis=1)\n",
        "\n",
        "print(tokenized_resource_sequence)\n",
        "# One-hot encode the labels\n",
        "\n",
        "\n",
        "vocab_size = np.unique(padded_resource_sets.flatten()).shape[0]\n",
        "embedding_dim = 64\n",
        "\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Embedding(input_dim=vocab_size + 1, output_dim=embedding_dim))\n",
        "model.add(LSTM(64, return_sequences=True))\n",
        "model.add(LSTM(64))\n",
        "model.add(Dense(labels.shape[1], activation='softmax'))\n",
        "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "print(tokenized_resource_sequence.shape)\n",
        "print(labels.shape)\n",
        "model.fit(tokenized_resource_sequence, labels, epochs=10, batch_size=32, validation_split=0.4)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6E6vg2oT7l7d"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
